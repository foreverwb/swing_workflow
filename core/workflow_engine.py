"""
WorkflowEngine - å·¥ä½œæµå¼•æ“
æ”¯æŒå®Œæ•´åˆ†ææµç¨‹å’Œç›˜ä¸­åˆ·æ–°
"""

import json
import time
import base64
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime
from loguru import logger

from core.model_client import ModelClientManager

import prompts
import schemas
from code_nodes import (
    event_detection_main,
    scoring_main,
    strategy_calc_main,
    comparison_main,
    aggregator_main
)


class WorkflowEngine:
    """å·¥ä½œæµå¼•æ“"""
    
    def __init__(self, model_client_manager: ModelClientManager, env_vars: Dict[str, Any]):
        """
        åˆå§‹åŒ–å·¥ä½œæµå¼•æ“
        
        Args:
            model_client_manager: æ¨¡å‹å®¢æˆ·ç«¯ç®¡ç†å™¨
            env_vars: ç¯å¢ƒå˜é‡å­—å…¸
        """
        self.model_client = model_client_manager
        self.env_vars = env_vars
        
        # ä¼šè¯çŠ¶æ€å˜é‡ï¼ˆç”¨äºå¢é‡è¡¥é½ï¼‰
        self.conversation_vars = {
            "missing_count": 0,
            "data_status": "initial",
            "current_symbol": "",
            "first_parse_data": ""
        }
        
        logger.info("å·¥ä½œæµå¼•æ“åˆå§‹åŒ–å®Œæˆ")
        
        # ç¼“å­˜æ–‡ä»¶
        self.cache_file = Path("data/temp") / "workflow_state.json"
        self.cache_file.parent.mkdir(parents=True, exist_ok=True)
        self._load_state()
    
    def _load_state(self):
        """ä»ç£ç›˜åŠ è½½ä¹‹å‰çš„åˆ†æçŠ¶æ€"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    self.conversation_vars = json.load(f)
                logger.info("ğŸ“‚ å·²åŠ è½½ä¹‹å‰çš„åˆ†æçŠ¶æ€ï¼Œæ”¯æŒå¢é‡è¡¥é½")
            except Exception as e:
                logger.warning(f"åŠ è½½çŠ¶æ€å¤±è´¥: {e}")
    
    def _save_state(self):
        """ä¿å­˜å½“å‰çŠ¶æ€åˆ°ç£ç›˜"""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.conversation_vars, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜çŠ¶æ€å¤±è´¥: {e}")
    
    def run(self, symbol: str, data_folder: Path, mode: str = "full") -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´å·¥ä½œæµ
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            data_folder: æ•°æ®æ–‡ä»¶å¤¹
            mode: è¿è¡Œæ¨¡å¼
                - "full": å®Œæ•´åˆ†æï¼ˆé»˜è®¤ï¼‰
                - "update": å¢é‡è¡¥é½æ•°æ®
                - "refresh": ä»…åˆ·æ–° Greeksï¼ˆç”¨äºç›˜ä¸­è§‚æµ‹ï¼‰
        
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        mode_desc = {
            "full": "å®Œæ•´åˆ†æ",
            "update": "å¢é‡è¡¥é½",
            "refresh": "åˆ·æ–°å¿«ç…§"
        }.get(mode, "å®Œæ•´åˆ†æ")
        
        logger.info(f"ğŸš€ å¼€å§‹{mode_desc} {symbol}")
        
        # 1. åŠ è½½å†å²ç¼“å­˜ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        cache_file = Path(f"data/cache/{symbol}_analysis.json")
        previous_data = None
        
        if mode in ["update", "refresh"] and cache_file.exists():
            with open(cache_file, 'r', encoding='utf-8') as f:
                cached = json.load(f)
                # ä»æœ€åä¸€ä¸ªå¿«ç…§è·å–å†å²æ•°æ®
                snapshots = cached.get("greeks_snapshots", [])
                if snapshots:
                    previous_data = snapshots[-1].get("data", {}).get("targets", {})
                    logger.info(f"ğŸ“‚ åŠ è½½å†å²å¿«ç…§æ•°æ®")
        
        # 2. æ‰«æå›¾ç‰‡
        image_paths = self._scan_folder_images(data_folder)
        if not image_paths:
            return {
                "status": "error",
                "message": f"æ–‡ä»¶å¤¹ {data_folder} ä¸­æœªæ‰¾åˆ°å›¾ç‰‡"
            }
        
        logger.info(f"ğŸ“Š æ‰«æåˆ° {len(image_paths)} å¼ å›¾ç‰‡ï¼Œå‡†å¤‡åˆ†æ")
        
        # 3. Agent 3 æ•°æ®æ ¡éªŒï¼ˆä¸€æ¬¡æ€§ä¸Šä¼ æ‰€æœ‰å›¾ç‰‡ï¼‰
        current_run_data = self._run_agent3_validate(
            symbol,
            image_paths,
            previous_data=previous_data
        )
        
        # 4. å¦‚æœæ˜¯ refresh æ¨¡å¼ï¼Œä¿å­˜å¿«ç…§å¹¶è¿”å›
        if mode == "refresh":
            # æ‰§è¡Œèšåˆå’Œè®¡ç®—
            aggregated_result = self._run_code_aggregator(current_run_data, symbol)
            aggregated_data = self._safe_parse_json(aggregated_result.get("result"))
            
            # æ‰§è¡Œå­—æ®µè®¡ç®—
            from code_nodes.field_calculator import main as calculator_main
            calculated_result = calculator_main(
                aggregated_data=aggregated_data,
                **self.env_vars
            )
            calculated_data = self._safe_parse_json(calculated_result.get("result"))
            
            # ä¿å­˜å¿«ç…§
            return self._save_greeks_snapshot(symbol, calculated_data)
        
        # 5. æ­£å¸¸æµç¨‹ï¼šè°ƒç”¨ Aggregator
        aggregated_result = self._run_code_aggregator(current_run_data, symbol)
        
        # è§£æèšåˆåçš„ç»“æœ
        final_data = self._safe_parse_json(aggregated_result.get("result"))
        data_status = aggregated_result.get("data_status")
        
        # 6. æ ¹æ®çŠ¶æ€å†³å®šåç»­æµç¨‹
        if data_status == "awaiting_data":
            logger.warning(f"âš ï¸ æ•°æ®ä»ç¼ºå¤± {aggregated_result.get('missing_count')} ä¸ªå­—æ®µï¼Œç”Ÿæˆè¡¥é½æŒ‡å¼•")
            return {
                "status": "incomplete",
                "guide": self._format_è¡¥é½æŒ‡å¼•(aggregated_result),
                "missing_count": aggregated_result.get("missing_count"),
                "merge_history": final_data.get("_merge_history", [])
            }
        
        elif data_status == "ready":
            logger.info("âœ… æ•°æ®å®Œæ•´ï¼Œå¼€å§‹åç»­åˆ†ææµç¨‹")
            return self._run_analysis_pipeline(final_data, cache_file)
        
        else:
            return {
                "status": "error",
                "message": f"æœªçŸ¥çš„æ•°æ®çŠ¶æ€: {data_status}"
            }
    
    def _scan_folder_images(self, folder: Path) -> List[Path]:
        """æ‰«ææ–‡ä»¶å¤¹è·å–æ‰€æœ‰æ”¯æŒçš„å›¾ç‰‡"""
        extensions = ['*.png', '*.PNG', '*.jpg', '*.JPG', '*.jpeg', '*.JPEG']
        image_paths = []
        for ext in extensions:
            image_paths.extend(list(folder.glob(ext)))
        return sorted(image_paths)
    
    def _encode_image_to_base64(self, image_path: Path) -> Optional[str]:
        """æœ¬åœ°å›¾ç‰‡è½¬ Base64"""
        try:
            with open(image_path, "rb") as image_file:
                base64_str = base64.b64encode(image_file.read()).decode('utf-8')
                ext = image_path.suffix.lower()
                mime_type = "image/jpeg" if ext in ['.jpg', '.jpeg'] else "image/png"
                return f"data:{mime_type};base64,{base64_str}"
        except Exception as e:
            logger.error(f"âŒ å›¾ç‰‡ç¼–ç å¤±è´¥ {image_path.name}: {e}")
            return None
    
    def _run_agent3_validate(
        self,
        symbol: str,
        image_paths: List[Path],
        previous_data: Optional[Dict] = None
    ) -> Dict:
        """
        Agent 3 æ•°æ®æ ¡éªŒï¼ˆä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰å›¾ç‰‡ï¼‰
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
            previous_data: å†å²æ•°æ®ï¼ˆç”¨äº update æ¨¡å¼ï¼‰
        
        Returns:
            è§£æåçš„æ•°æ®å­—å…¸
        """
        logger.info(f"ğŸ”„ [Agent3] å¤„ç† {len(image_paths)} å¼ å›¾ç‰‡...")
        
        # 1. æ„å»º Prompt
        system_content = prompts.agent3_validate.get_system_prompt(self.env_vars)
        
        # å¦‚æœæœ‰å†å²æ•°æ®ï¼Œæ·»åŠ ä¸Šä¸‹æ–‡
        if previous_data:
            system_content += f"""

ã€é‡è¦ï¼šè¿™æ˜¯å¢é‡è¡¥é½ä»»åŠ¡ã€‘
ä»¥ä¸‹æ˜¯é¦–æ¬¡åˆ†æå·²è·å–çš„æ•°æ®ï¼Œè¯·åœ¨æ­¤åŸºç¡€ä¸Šè¡¥å……ç¼ºå¤±å­—æ®µï¼š
```json
{json.dumps(previous_data, ensure_ascii=False, indent=2)}
```

**è¡¥é½è¦æ±‚**ï¼š
1. ä¿ç•™ä¸Šè¿°å·²æœ‰çš„æœ‰æ•ˆæ•°æ®ï¼ˆä¸è¦è¦†ç›–ï¼‰
2. ä»å½“å‰ä¸Šä¼ çš„å›¾ç‰‡ä¸­æå–ç¼ºå¤±çš„å­—æ®µ
3. å¯¹äºéœ€è¦å…³è”è®¡ç®—çš„å­—æ®µï¼Œæå–åŸå§‹æ•°æ®å³å¯ï¼ˆç³»ç»Ÿä¼šè‡ªåŠ¨è®¡ç®—ï¼‰
4. ç¡®ä¿æ‰€æœ‰ 22 ä¸ªå¿…éœ€å­—æ®µéƒ½æœ‰æœ‰æ•ˆå€¼
"""
        
        user_prompt = prompts.agent3_validate.get_user_prompt(
            symbol,
            [p.name for p in image_paths]
        )
        
        # 2. æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        inputs = [
            {"role": "system", "content": system_content},
            {"role": "user", "content": user_prompt}
        ]
        
        # 3. æ·»åŠ æ‰€æœ‰å›¾ç‰‡
        valid_img_count = 0
        for path in image_paths:
            b64_str = self._encode_image_to_base64(path)
            if b64_str:
                inputs.append({
                    "role": "user",
                    "content": [{"type": "image_url", "image_url": {"url": b64_str}}]
                })
                valid_img_count += 1
        
        if valid_img_count == 0:
            logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆå›¾ç‰‡å¯å¤„ç†")
            return {}
        
        logger.info(f"ğŸ“¸ å·²ç¼–ç  {valid_img_count} å¼ å›¾ç‰‡")
        
        # 4. è°ƒç”¨ API
        try:
            response = self.model_client.responses_create(
                inputs=inputs,
                agent_name="agent3",
                json_schema=schemas.agent3_schema.get_schema()
            )
            
            # â­ æ–°å¢ï¼šæ‰“å° Agent3 è¿”å›æ•°æ®
            self._print_agent_response("Agent3 - æ•°æ®æ ¡éªŒ", response)
            
            # 5. è§£æå“åº”
            raw_content = response.get("content", {})
            
            # è°ƒè¯•æ—¥å¿—
            logger.info(f"ğŸ“Š å“åº”ç±»å‹: {type(raw_content)}")
            logger.info(f"ğŸ“Š å“åº”é•¿åº¦: {len(str(raw_content))} å­—ç¬¦")
            
            # è§£æ JSON
            if isinstance(raw_content, dict):
                batch_data = raw_content
            elif isinstance(raw_content, str):
                # å°è¯•æ¸…æ´— Markdown æ ‡è®°
                try:
                    clean_text = raw_content.strip()
                    if clean_text.startswith("```json"):
                        clean_text = clean_text[7:]
                    if clean_text.startswith("```"):
                        clean_text = clean_text[3:]
                    if clean_text.endswith("```"):
                        clean_text = clean_text[:-3]
                    batch_data = json.loads(clean_text.strip())
                except json.JSONDecodeError as e:
                    logger.error(f"âŒ JSON è§£æå¤±è´¥: {str(e)}")
                    logger.debug(f"åŸå§‹å†…å®¹ç‰‡æ®µ: {raw_content[:200]}")
                    return {}
            else:
                logger.error(f"âŒ æœªçŸ¥çš„å“åº”ç±»å‹: {type(raw_content)}")
                return {}
            
            logger.success(f"âœ… æ•°æ®è§£ææˆåŠŸ")
            
            # â­ æ–°å¢ï¼šæ‰“å°è§£æåçš„æ•°æ®æ‘˜è¦
            self._print_data_summary("Agent3 è§£æç»“æœ", batch_data)
            
            return batch_data
        
        except Exception as e:
            logger.error(f"âŒ Agent3 è°ƒç”¨å¤±è´¥: {e}")
            return {}
    
    def _run_code_aggregator(self, current_run_data: Dict, symbol: str) -> Dict:
        """è°ƒç”¨ Aggregator èŠ‚ç‚¹è¿›è¡Œè·¨è½®æ¬¡æ•°æ®ç´¯ç§¯"""
        logger.info("ğŸ“¦ [Aggregator] æ‰§è¡Œæ•°æ®èšåˆ")
        
        result = aggregator_main(
            agent3_output=current_run_data,
            first_parse_data=self.conversation_vars["first_parse_data"],
            current_symbol=symbol,
            data_status=self.conversation_vars["data_status"],
            missing_count=self.conversation_vars["missing_count"],
            **self.env_vars
        )
        
        # â­ æ–°å¢ï¼šæ‰“å° Aggregator ç»“æœ
        self._print_code_node_result("Aggregator", result)
        
        # æ›´æ–°ä¼šè¯çŠ¶æ€ï¼ˆå®ç°è®°å¿†åŠŸèƒ½ï¼‰
        if "first_parse_data" in result:
            self.conversation_vars["first_parse_data"] = result["first_parse_data"]
        if "data_status" in result:
            self.conversation_vars["data_status"] = result["data_status"]
        if "missing_count" in result:
            self.conversation_vars["missing_count"] = result["missing_count"]
        
        self._save_state()
        return result
    
    def _format_è¡¥é½æŒ‡å¼•(self, result: Dict) -> str:
        """æ ¼å¼åŒ– Aggregator è¿”å›çš„æŒ‡å¼•ä¿¡æ¯"""
        return f"""
==================================================
ğŸ“‹ æ•°æ®è¡¥é½æŒ‡å¼• ({result.get('user_guide_progress', '0%')})
==================================================

{result.get('user_guide_summary', '')}

ğŸ”´ å¿…é¡»è¡¥é½ (Critical):
{result.get('user_guide_priority_critical', 'æ— ')}

ğŸŸ  å»ºè®®è¡¥é½ (High):
{result.get('user_guide_priority_high', 'æ— ')}

ğŸŸ¡ å¯é€‰è¡¥é½ (Medium):
{result.get('user_guide_priority_medium', 'æ— ')}

ğŸ“ å†å²åˆå¹¶è®°å½•:
{result.get('user_guide_merge_log', '')}

ğŸ‘‰ ä¸‹ä¸€æ­¥: {result.get('user_guide_next_action', '')}
"""
    
    def _run_analysis_pipeline(self, aggregated_result: Dict, cache_file: Path) -> Dict:
        """
        è¿è¡Œå®Œæ•´åˆ†ææµç¨‹
        
        Args:
            aggregated_result: èšåˆåçš„æ•°æ®
            cache_file: ç¼“å­˜æ–‡ä»¶è·¯å¾„
        """
        # è§£æèšåˆæ•°æ®
        merged_data = self._safe_parse_json(aggregated_result.get("result"))
        
        # â­ æ–°å¢ï¼šå…³è”å­—æ®µè®¡ç®—
        logger.info("ğŸ§® [Calculator] æ‰§è¡Œå…³è”å­—æ®µè®¡ç®—")
        from code_nodes.field_calculator import main as calculator_main
        
        calculated_result = calculator_main(
            aggregated_data=merged_data,
            **self.env_vars
        )
        
        # â­ æ–°å¢ï¼šæ‰“å°è®¡ç®—ç»“æœ
        self._print_code_node_result("Calculator", calculated_result)
        
        calculated_data = self._safe_parse_json(calculated_result.get("result"))
        
        # éªŒè¯è®¡ç®—ç»“æœ
        calc_log = calculated_data.get("targets", {}).get("_calculation_log", {})
        if calc_log.get("checks"):
            logger.info(f"ğŸ“Š è®¡ç®—éªŒè¯: {len(calc_log['checks'])} é¡¹æ£€æŸ¥")
            for check in calc_log["checks"]:
                if not check.get("is_valid"):
                    logger.warning(f"âš ï¸ {check['field']}: {check['note']}")
        
        # Step 1: CODE1 äº‹ä»¶æ£€æµ‹
        logger.info("ğŸ” Step 1: äº‹ä»¶æ£€æµ‹")
        event_result = event_detection_main(
            user_query=f"åˆ†æ {calculated_data.get('symbol', 'UNKNOWN')}",
            **self.env_vars
        )
        
        # â­ æ–°å¢ï¼šæ‰“å°äº‹ä»¶æ£€æµ‹ç»“æœ
        self._print_code_node_result("CODE1 - äº‹ä»¶æ£€æµ‹", event_result)
        
        # Step 2: CODE2 è¯„åˆ†è®¡ç®—
        logger.info("ğŸ“Š Step 2: å››ç»´è¯„åˆ†")
        scoring_result = scoring_main(
            agent3_output=calculated_data,
            technical_score=calculated_data.get("technical_analysis", {}).get("ta_score", 0),
            **self.env_vars
        )
        
        # â­ æ–°å¢ï¼šæ‰“å°è¯„åˆ†ç»“æœ
        self._print_code_node_result("CODE2 - è¯„åˆ†è®¡ç®—", scoring_result)
        
        scoring_data = self._safe_parse_json(scoring_result.get("result"))
        
        # Step 3: Agent 5 åœºæ™¯åˆ†æ
        logger.info("ğŸ¯ Step 3: åœºæ™¯æ¨æ¼”")
        agent5_result = self._run_agent5_scenario(scoring_data)
        
        # â­ æ–°å¢ï¼šæ‰“å°åœºæ™¯åˆ†æç»“æœ
        self._print_agent_response("Agent5 - åœºæ™¯åˆ†æ", agent5_result)
        
        # Step 4: CODE3 ç­–ç•¥è¾…åŠ©è®¡ç®—
        logger.info("ğŸ§® Step 4: ç­–ç•¥è¾…åŠ©")
        strategy_calc_result = strategy_calc_main(
            agent3_output=calculated_data,
            agent5_output=agent5_result["content"],
            technical_score=calculated_data.get("technical_analysis", {}).get("ta_score", 0),
            **self.env_vars
        )
        
        # â­ æ–°å¢ï¼šæ‰“å°ç­–ç•¥è¾…åŠ©è®¡ç®—ç»“æœ
        self._print_code_node_result("CODE3 - ç­–ç•¥è¾…åŠ©", strategy_calc_result)
        
        strategy_calc_data = self._safe_parse_json(strategy_calc_result.get("result"))
        
        # Step 5: Agent 6 ç­–ç•¥ç”Ÿæˆ
        logger.info("ğŸ’¡ Step 5: ç­–ç•¥ç”Ÿæˆ")
        agent6_result = self._run_agent6_strategy(
            agent5_result,
            strategy_calc_data,
            calculated_data
        )
        
        # â­ æ–°å¢ï¼šæ‰“å°ç­–ç•¥ç”Ÿæˆç»“æœ
        self._print_agent_response("Agent6 - ç­–ç•¥ç”Ÿæˆ", agent6_result)
        
        # Step 6: CODE4 ç­–ç•¥å¯¹æ¯”
        logger.info("âš–ï¸ Step 6: ç­–ç•¥å¯¹æ¯”")
        comparison_result = comparison_main(
            strategies_output=agent6_result["content"],
            scenario_output=agent5_result["content"],
            agent3_output=calculated_data,
            **self.env_vars
        )
        
        # â­ æ–°å¢ï¼šæ‰“å°ç­–ç•¥å¯¹æ¯”ç»“æœ
        self._print_code_node_result("CODE4 - ç­–ç•¥å¯¹æ¯”", comparison_result)
        
        comparison_data = self._safe_parse_json(comparison_result.get("result"))
        
        # Step 7: Agent 7 ç­–ç•¥æ’åº
        logger.info("ğŸ† Step 7: ç­–ç•¥æ’åº")
        agent7_result = self._run_agent7_comparison(
            comparison_data,
            agent5_result["content"],
            agent6_result["content"]
        )
        
        # â­ æ–°å¢ï¼šæ‰“å°ç­–ç•¥æ’åºç»“æœ
        self._print_agent_response("Agent7 - ç­–ç•¥æ’åº", agent7_result)
        
        # Step 8: Agent 8 æœ€ç»ˆæŠ¥å‘Š
        logger.info("ğŸ“‹ Step 8: ç”ŸæˆæŠ¥å‘Š")
        final_report = self._run_agent8_report(
            calculated_data,
            agent5_result["content"],
            agent7_result["content"],
            event_result
        )
        
        # â­ æ–°å¢ï¼šæ‰“å°æœ€ç»ˆæŠ¥å‘Šï¼ˆä»…å‰500å­—ç¬¦ï¼‰
        self._print_agent_response("Agent8 - æœ€ç»ˆæŠ¥å‘Š", final_report, truncate=500)
        
        # ä¿å­˜å®Œæ•´åˆ†æç»“æœåˆ°ç¼“å­˜
        self._save_complete_analysis(
            cache_file=cache_file,
            symbol=calculated_data.get("symbol", "UNKNOWN"),
            initial_data=calculated_data,
            scenario=agent5_result["content"],
            strategies=agent6_result["content"],
            ranking=agent7_result["content"],
            report=final_report["content"]
        )
        
        logger.success("âœ… åˆ†æå®Œæˆ!")
        
        return {
            "status": "success",
            "report": final_report["content"],
            "event_risk": self._safe_parse_json(event_result.get("result")),
            "scoring": scoring_data,
            "scenario": agent5_result["content"],
            "strategies": agent6_result["content"],
            "ranking": agent7_result["content"]
        }
    
    def _run_agent5_scenario(self, scoring_data: Dict) -> Dict:
        """Agent 5: åœºæ™¯åˆ†æ"""
        messages = [
            {
                "role": "system",
                "content": prompts.agent5_scenario.get_system_prompt()
            },
            {
                "role": "user",
                "content": prompts.agent5_scenario.get_user_prompt(scoring_data)
            }
        ]
        
        response = self.model_client.chat_completion(
            messages=messages,
            agent_name="agent5",
            json_schema=schemas.agent5_schema.get_schema()
        )
        
        return response
    
    def _run_agent6_strategy(self, agent5_result: Dict, calc_data: Dict, agent3_data: Dict) -> Dict:
        """Agent 6: ç­–ç•¥ç”Ÿæˆ"""
        messages = [
            {
                "role": "system",
                "content": prompts.agent6_strategy.get_system_prompt(self.env_vars)
            },
            {
                "role": "user",
                "content": prompts.agent6_strategy.get_user_prompt(
                    agent5_result,
                    calc_data,
                    agent3_data
                )
            }
        ]
        
        response = self.model_client.chat_completion(
            messages=messages,
            agent_name="agent6",
            json_schema=schemas.agent6_schema.get_schema()
        )
        
        return response
    
    def _run_agent7_comparison(self, comparison_data: Dict, scenario: Dict, strategies: Dict) -> Dict:
        """Agent 7: ç­–ç•¥å¯¹æ¯”"""
        messages = [
            {
                "role": "system",
                "content": prompts.agent7_comparison.get_system_prompt()
            },
            {
                "role": "user",
                "content": prompts.agent7_comparison.get_user_prompt(
                    comparison_data, scenario, strategies
                )
            }
        ]
        
        response = self.model_client.chat_completion(
            messages=messages,
            agent_name="agent7",
            json_schema=schemas.agent7_schema.get_schema()
        )
        
        return response
    
    def _run_agent8_report(self, agent3: Dict, agent5: Dict, agent7: Dict, event: Dict) -> Dict:
        """Agent 8: æœ€ç»ˆæŠ¥å‘Š"""
        messages = [
            {
                "role": "system",
                "content": prompts.agent8_report.get_system_prompt()
            },
            {
                "role": "user",
                "content": prompts.agent8_report.get_user_prompt(
                    agent3, agent5, agent7, event
                )
            }
        ]
        
        response = self.model_client.chat_completion(
            messages=messages,
            agent_name="agent8"
        )
        
        return response
    
    # â­â­â­ æ–°å¢ï¼šæ‰“å°è¾…åŠ©å‡½æ•° â­â­â­
    
    def _print_agent_response(self, agent_name: str, response: Dict, truncate: int = None):
        """
        æ‰“å° Agent å“åº”æ•°æ®
        
        Args:
            agent_name: Agent åç§°
            response: å“åº”å­—å…¸
            truncate: æˆªæ–­é•¿åº¦ï¼ˆå¯é€‰ï¼Œç”¨äºé•¿æ–‡æœ¬ï¼‰
        """
        print("\n" + "="*80)
        print(f"ğŸ“¤ {agent_name} è¿”å›æ•°æ®")
        print("="*80)
        
        # æ‰“å°å…ƒæ•°æ®
        if "model" in response:
            print(f"ğŸ¤– æ¨¡å‹: {response['model']}")
        if "usage" in response:
            usage = response["usage"]
            print(f"ğŸ“Š Tokenä½¿ç”¨: è¾“å…¥={usage.get('input_tokens', 0)}, è¾“å‡º={usage.get('output_tokens', 0)}")
        
        # æ‰“å°å†…å®¹
        content = response.get("content", {})
        
        if isinstance(content, dict):
            print(f"\nğŸ“‹ å†…å®¹ç±»å‹: dict")
            print(f"ğŸ“‹ å­—æ®µæ•°é‡: {len(content)}")
            
            # æ‰“å°ä¸»è¦å­—æ®µ
            if truncate:
                content_str = json.dumps(content, ensure_ascii=False, indent=2)
                if len(content_str) > truncate:
                    print(f"\n{content_str[:truncate]}...")
                    print(f"\n[å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­è‡³ {truncate} å­—ç¬¦]")
                else:
                    print(f"\n{content_str}")
            else:
                # æ‰“å°å…³é”®å­—æ®µæ‘˜è¦
                key_fields = ["symbol", "status", "total_score", "scenario_classification", "strategies"]
                print(f"\nğŸ”‘ å…³é”®å­—æ®µ:")
                for key in key_fields:
                    if key in content:
                        value = content[key]
                        if isinstance(value, (dict, list)):
                            print(f"  â€¢ {key}: {type(value).__name__} (é•¿åº¦: {len(value)})")
                        else:
                            print(f"  â€¢ {key}: {value}")
        
        elif isinstance(content, str):
            print(f"\nğŸ“‹ å†…å®¹ç±»å‹: str")
            print(f"ğŸ“‹ å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            if truncate and len(content) > truncate:
                print(f"\n{content[:truncate]}...")
                print(f"\n[å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­è‡³ {truncate} å­—ç¬¦]")
            else:
                print(f"\n{content}")
        
        else:
            print(f"\nğŸ“‹ å†…å®¹ç±»å‹: {type(content)}")
            print(f"\n{content}")
        
        print("="*80 + "\n")
    
    def _print_code_node_result(self, node_name: str, result: Dict):
        """
        æ‰“å° Code Node ç»“æœ
        
        Args:
            node_name: èŠ‚ç‚¹åç§°
            result: ç»“æœå­—å…¸
        """
        print("\n" + "="*80)
        print(f"ğŸ”§ {node_name} æ‰§è¡Œç»“æœ")
        print("="*80)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        if "error" in result or (isinstance(result.get("result"), str) and "error" in result["result"]):
            print(f"âŒ æ‰§è¡Œå¤±è´¥")
            print(f"\n{json.dumps(result, ensure_ascii=False, indent=2)}")
            print("="*80 + "\n")
            return
        
        # æ‰“å°ç»“æœ
        result_data = result.get("result", {})
        
        if isinstance(result_data, str):
            # å°è¯•è§£æ JSON
            try:
                parsed = json.loads(result_data)
                print(f"ğŸ“‹ ç»“æœç±»å‹: JSON (å·²è§£æ)")
                
                # æ‰“å°å…³é”®ä¿¡æ¯
                if isinstance(parsed, dict):
                    print(f"ğŸ“‹ å­—æ®µæ•°é‡: {len(parsed)}")
                    
                    # æå–å…³é”®å­—æ®µ
                    key_indicators = [
                        "symbol", "status", "data_status", "missing_count",
                        "validation_summary", "total_score", "em1_dollar",
                        "calculation_log", "event_count", "risk_level"
                    ]
                    
                    print(f"\nğŸ”‘ å…³é”®æŒ‡æ ‡:")
                    for key in key_indicators:
                        if key in parsed:
                            value = parsed[key]
                            if isinstance(value, dict):
                                print(f"  â€¢ {key}: {json.dumps(value, ensure_ascii=False)}")
                            else:
                                print(f"  â€¢ {key}: {value}")
                
                # æ‰“å°å‰500å­—ç¬¦çš„å®Œæ•´JSON
                full_json = json.dumps(parsed, ensure_ascii=False, indent=2)
                if len(full_json) > 500:
                    print(f"\nğŸ“„ å®Œæ•´æ•°æ®ï¼ˆå‰500å­—ç¬¦ï¼‰:")
                    print(full_json[:500] + "...")
                else:
                    print(f"\nğŸ“„ å®Œæ•´æ•°æ®:")
                    print(full_json)
                    
            except json.JSONDecodeError:
                print(f"ğŸ“‹ ç»“æœç±»å‹: str (éJSON)")
                print(f"ğŸ“‹ å†…å®¹é•¿åº¦: {len(result_data)} å­—ç¬¦")
                if len(result_data) > 500:
                    print(f"\n{result_data[:500]}...")
                else:
                    print(f"\n{result_data}")
        
        elif isinstance(result_data, dict):
            print(f"ğŸ“‹ ç»“æœç±»å‹: dict")
            print(f"ğŸ“‹ å­—æ®µæ•°é‡: {len(result_data)}")
            print(f"\n{json.dumps(result_data, ensure_ascii=False, indent=2)[:500]}...")
        else:
            print(f"ğŸ“‹ ç»“æœç±»å‹: {type(result_data)}")
            print(f"\n{result_data}")
        
        print("="*80 + "\n")
    
    def _print_data_summary(self, title: str, data: Dict):
        """
        æ‰“å°æ•°æ®æ‘˜è¦
        
        Args:
            title: æ ‡é¢˜
            data: æ•°æ®å­—å…¸
        """
        print("\n" + "="*80)
        print(f"ğŸ“Š {title}")
        print("="*80)
        
        if not isinstance(data, dict):
            print(f"âš ï¸ æ•°æ®ç±»å‹é”™è¯¯: {type(data)}")
            print("="*80 + "\n")
            return
        
        # æå–å…³é”®ä¿¡æ¯
        if "targets" in data:
            targets = data["targets"]
            if isinstance(targets, dict):
                print(f"âœ… targets ç±»å‹: dict")
                print(f"âœ… Symbol: {targets.get('symbol', 'N/A')}")
                print(f"âœ… Status: {targets.get('status', 'N/A')}")
                print(f"âœ… Spot Price: {targets.get('spot_price', 'N/A')}")
                print(f"âœ… EM1 Dollar: {targets.get('em1_dollar', 'N/A')}")
                
                # æ£€æŸ¥åµŒå¥—å­—æ®µ
                if "gamma_metrics" in targets:
                    gm = targets["gamma_metrics"]
                    print(f"\nğŸ“ˆ Gamma Metrics:")
                    print(f"  â€¢ vol_trigger: {gm.get('vol_trigger', 'N/A')}")
                    print(f"  â€¢ spot_vs_trigger: {gm.get('spot_vs_trigger', 'N/A')}")
                    print(f"  â€¢ net_gex: {gm.get('net_gex', 'N/A')}")
                
                if "walls" in targets:
                    walls = targets["walls"]
                    print(f"\nğŸ§± Walls:")
                    print(f"  â€¢ call_wall: {walls.get('call_wall', 'N/A')}")
                    print(f"  â€¢ put_wall: {walls.get('put_wall', 'N/A')}")
                    print(f"  â€¢ major_wall: {walls.get('major_wall', 'N/A')}")
            else:
                print(f"âš ï¸ targets ç±»å‹: {type(targets)}")
        
        if "validation_summary" in data:
            vs = data["validation_summary"]
            print(f"\nâœ”ï¸ éªŒè¯æ‘˜è¦:")
            print(f"  â€¢ å®Œæˆç‡: {vs.get('completion_rate', 0)}%")
            print(f"  â€¢ æä¾›å­—æ®µ: {vs.get('provided', 0)}/{vs.get('total_required', 22)}")
            print(f"  â€¢ ç¼ºå¤±å­—æ®µ: {vs.get('missing_count', 0)}")
        
        print("="*80 + "\n")
    
    # â­â­â­ æ‰“å°è¾…åŠ©å‡½æ•°ç»“æŸ â­â­â­
    
    def _save_complete_analysis(
        self,
        cache_file: Path,
        symbol: str,
        initial_data: Dict,
        scenario: Dict,
        strategies: Dict,
        ranking: Dict,
        report: str
    ):
        """
        ä¿å­˜å®Œæ•´åˆ†æç»“æœåˆ°ç¼“å­˜
        
        Args:
            cache_file: ç¼“å­˜æ–‡ä»¶è·¯å¾„
            symbol: è‚¡ç¥¨ä»£ç 
            initial_data: åˆå§‹æ•°æ®
            scenario: åœºæ™¯åˆ†æ
            strategies: ç­–ç•¥åˆ—è¡¨
            ranking: ç­–ç•¥æ’åº
            report: æœ€ç»ˆæŠ¥å‘Š
        """
        cache_file.parent.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½ç°æœ‰ç¼“å­˜ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if cache_file.exists():
            with open(cache_file, 'r', encoding='utf-8') as f:
                cached = json.load(f)
        else:
            cached = {
                "symbol": symbol,
                "created_at": datetime.now().isoformat(),
                "last_updated": None,
                "analysis": {},
                "greeks_snapshots": [],
                "backtest_records": []
            }
        
        # æ›´æ–°åˆ†æç»“æœ
        cached["analysis"] = {
            "status": "completed",
            "initial_date": datetime.now().strftime("%Y-%m-%d"),
            "initial_spot": initial_data.get("targets", {}).get("spot_price"),
            "scenario": scenario,
            "strategies": strategies,
            "ranking": ranking,
            "report": report
        }
        
        cached["last_updated"] = datetime.now().isoformat()
        
        # ä¿å­˜é¦–æ¬¡å¿«ç…§
        if not cached["greeks_snapshots"]:
            snapshot = {
                "snapshot_id": 0,
                "type": "initial_analysis",
                "timestamp": datetime.now().isoformat(),
                "note": "å®Œæ•´åˆ†æ",
                "spot_price": initial_data.get("targets", {}).get("spot_price"),
                "em1_dollar": initial_data.get("targets", {}).get("em1_dollar"),
                "vol_trigger": self._get_nested_value(initial_data.get("targets", {}), "gamma_metrics.vol_trigger"),
                "spot_vs_trigger": self._get_nested_value(initial_data.get("targets", {}), "gamma_metrics.spot_vs_trigger"),
                "net_gex": self._get_nested_value(initial_data.get("targets", {}), "gamma_metrics.net_gex"),
                "call_wall": self._get_nested_value(initial_data.get("targets", {}), "walls.call_wall"),
                "put_wall": self._get_nested_value(initial_data.get("targets", {}), "walls.put_wall"),
                "iv_7d": self._get_nested_value(initial_data.get("targets", {}), "atm_iv.iv_7d"),
                "iv_14d": self._get_nested_value(initial_data.get("targets", {}), "atm_iv.iv_14d"),
                "data": initial_data,
                "changes": None
            }
            cached["greeks_snapshots"].append(snapshot)
        
        # ä¿å­˜ç¼“å­˜
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cached, f, ensure_ascii=False, indent=2)
        
        logger.success(f"âœ… å®Œæ•´åˆ†æç»“æœå·²ä¿å­˜: {cache_file}")
    
    def _save_greeks_snapshot(self, symbol: str, data: Dict, note: str = "") -> Dict:
        """
        ä¿å­˜ Greeks å¿«ç…§ï¼ˆç”¨äº refresh æ¨¡å¼ï¼‰
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            data: å®Œæ•´æ•°æ®ï¼ˆå«è®¡ç®—ç»“æœï¼‰
            note: å¿«ç…§å¤‡æ³¨
        
        Returns:
            å¿«ç…§ä¿å­˜ç»“æœ
        """
        cache_file = Path(f"data/cache/{symbol}_analysis.json")
        cache_file.parent.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½ç°æœ‰ç¼“å­˜
        if cache_file.exists():
            with open(cache_file, 'r', encoding='utf-8') as f:
                cached = json.load(f)
        else:
            cached = {
                "symbol": symbol,
                "created_at": datetime.now().isoformat(),
                "last_updated": None,
                "analysis": {},
                "greeks_snapshots": [],
                "backtest_records": []
            }
        
        # æå–å…³é”®æ•°æ®
        targets = data.get("targets", {})
        
        # è·å–ä¸Šä¸€æ¬¡å¿«ç…§
        previous_snapshot = cached["greeks_snapshots"][-1] if cached["greeks_snapshots"] else None
        
        # åˆ›å»ºæ–°å¿«ç…§
        snapshot_id = len(cached["greeks_snapshots"])
        new_snapshot = {
            "snapshot_id": snapshot_id,
            "type": "initial_analysis" if snapshot_id == 0 else "intraday_refresh",
            "timestamp": datetime.now().isoformat(),
            "note": note,
            "spot_price": targets.get("spot_price"),
            "em1_dollar": targets.get("em1_dollar"),
            "vol_trigger": self._get_nested_value(targets, "gamma_metrics.vol_trigger"),
            "spot_vs_trigger": self._get_nested_value(targets, "gamma_metrics.spot_vs_trigger"),
            "net_gex": self._get_nested_value(targets, "gamma_metrics.net_gex"),
            "call_wall": self._get_nested_value(targets, "walls.call_wall"),
            "put_wall": self._get_nested_value(targets, "walls.put_wall"),
            "iv_7d": self._get_nested_value(targets, "atm_iv.iv_7d"),
            "iv_14d": self._get_nested_value(targets, "atm_iv.iv_14d"),
            "data": data,
            "changes": None
        }
        
        # è®¡ç®—å˜åŒ–
        if previous_snapshot:
            new_snapshot["changes"] = self._calculate_snapshot_changes(
                previous_snapshot,
                new_snapshot
            )
        
        # æ·»åŠ å¿«ç…§
        cached["greeks_snapshots"].append(new_snapshot)
        cached["last_updated"] = datetime.now().isoformat()
        
        # ä¿å­˜ç¼“å­˜
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cached, f, ensure_ascii=False, indent=2)
        
        logger.success(f"âœ… å¿«ç…§å·²ä¿å­˜: {cache_file}")
        
        # ç”Ÿæˆæ‘˜è¦
        summary = self._generate_snapshot_summary(new_snapshot)
        
        return {
            "status": "success",
            "snapshot": new_snapshot,
            "snapshot_summary": summary,
            "cache_file": str(cache_file)
        }
    
    def _calculate_snapshot_changes(self, old_snapshot: Dict, new_snapshot: Dict) -> Dict:
        """è®¡ç®—ä¸¤æ¬¡å¿«ç…§çš„å˜åŒ–"""
        changes = {}
        
        key_fields = [
            "spot_price", "em1_dollar", "vol_trigger",
            "call_wall", "put_wall", "net_gex",
            "iv_7d", "iv_14d"
        ]
        
        for field in key_fields:
            old_value = old_snapshot.get(field)
            new_value = new_snapshot.get(field)
            
            if old_value is None or new_value is None:
                continue
            
            if old_value == -999 or new_value == -999:
                continue
            
            if old_value != new_value:
                change_info = {
                    "old": old_value,
                    "new": new_value
                }
                
                # è®¡ç®—ç™¾åˆ†æ¯”å˜åŒ–
                if isinstance(old_value, (int, float)) and isinstance(new_value, (int, float)):
                    if old_value != 0:
                        pct_change = ((new_value - old_value) / old_value) * 100
                        change_info["change_pct"] = round(pct_change, 2)
                
                changes[field] = change_info
        
        return changes if changes else None
    
    def _generate_snapshot_summary(self, snapshot: Dict) -> str:
        """ç”Ÿæˆå¿«ç…§æ‘˜è¦"""
        lines = [
            f"å¿«ç…§ #{snapshot['snapshot_id']}",
            f"æ—¶é—´: {snapshot['timestamp'][:19]}",
            f"ç±»å‹: {snapshot['type']}",
            ""
        ]
        
        if snapshot.get('note'):
            lines.append(f"å¤‡æ³¨: {snapshot['note']}")
            lines.append("")
        
        lines.extend([
            f"ç°ä»·: ${snapshot.get('spot_price', 'N/A')}",
            f"EM1$: ${snapshot.get('em1_dollar', 'N/A')}",
            f"Vol Trigger: ${snapshot.get('vol_trigger', 'N/A')}",
            f"çŠ¶æ€: {snapshot.get('spot_vs_trigger', 'N/A')}",
            f"NET-GEX: {snapshot.get('net_gex', 'N/A')}",
            ""
        ])
        
        if snapshot.get('changes'):
            lines.append("å˜åŒ–:")
            for field, change in snapshot['changes'].items():
                pct_str = f" ({change['change_pct']:+.2f}%)" if 'change_pct' in change else ""
                lines.append(f"  â€¢ {field}: {change['old']} â†’ {change['new']}{pct_str}")
        
        return "\n".join(lines)
    
    def _get_nested_value(self, data: Dict, path: str):
        """è·å–åµŒå¥—å­—æ®µå€¼ï¼ˆæ”¯æŒç‚¹å·è·¯å¾„ï¼‰"""
        keys = path.split('.')
        value = data
        for key in keys:
            if isinstance(value, dict):
                value = value.get(key)
            else:
                return None
        return value if value != -999 else None
    
    def _safe_parse_json(self, data: Any) -> Dict:
        """å®‰å…¨è§£æJSON"""
        if isinstance(data, dict):
            return data
        elif isinstance(data, str):
            try:
                return json.loads(data)
            except json.JSONDecodeError as e:
                logger.error(f"JSONè§£æå¤±è´¥: {str(e)[:100]}")
                logger.debug(f"åŸå§‹æ•°æ®: {data[:200]}")
                return {}
        else:
            logger.warning(f"æœªçŸ¥æ•°æ®ç±»å‹: {type(data)}")
            return {}