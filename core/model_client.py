"""
æ¨¡å‹å®¢æˆ·ç«¯å°è£…
ä»…æ”¯æŒ OpenAI å…¼å®¹æ¥å£ï¼ˆåŒ…æ‹¬ Responses APIï¼‰
â­ æ–°å¢ï¼šæ”¯æŒ Strict JSON Schema Mode
"""

import os
import json
from typing import Dict, Any, List, Optional
from loguru import logger
import copy
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

def _sanitize_json_schema_for_vision(schema: Dict[str, Any]) -> Dict[str, Any]:
        """
        é€’å½’è§„èŒƒåŒ– JSON Schema ä»¥æ»¡è¶³ vision_structured_output çš„è¦æ±‚ï¼š
        - å¯¹å« properties çš„ object èŠ‚ç‚¹ï¼Œç¡®ä¿ additionalProperties=Falseï¼ˆè‹¥æœªæä¾›ï¼‰ã€‚
        - å¯¹å« properties çš„ object èŠ‚ç‚¹ï¼Œç¡®ä¿ required æ˜¯æ•°ç»„å¹¶åŒ…å« properties ä¸­çš„æ¯ä¸ªé”®ï¼ˆè¡¥é½ç¼ºå¤±é¡¹ï¼‰ã€‚
        è¿”å›æ·±æ‹·è´åçš„ schemaï¼Œä¸ä¿®æ”¹åŸå¯¹è±¡ã€‚
        """
        def _rec(node):
            if not isinstance(node, dict):
                return node

            node = dict(node)  # shallow copy for safety

            node_type = node.get("type")
            has_props = isinstance(node.get("properties"), dict)

            # å¦‚æœæ˜¯ object ç±»å‹æˆ–å« propertiesï¼Œåˆ™è§†ä¸º object èŠ‚ç‚¹
            if node_type == "object" or has_props:
                # additionalProperties è¦æ˜¾å¼ä¸º Falseï¼ˆè‹¥ä¸º dictï¼Œé€’å½’ï¼‰
                if "additionalProperties" not in node:
                    node["additionalProperties"] = False
                elif isinstance(node["additionalProperties"], dict):
                    node["additionalProperties"] = _rec(node["additionalProperties"])

                # è§„èŒƒ requiredï¼šå¿…é¡»å­˜åœ¨ä¸”åŒ…å«æ‰€æœ‰ properties çš„é”®
                if has_props:
                    prop_keys = list(node["properties"].keys())
                    existing_required = node.get("required")
                    if isinstance(existing_required, list):
                        # è¡¥é½ç¼ºå¤±çš„ keys
                        missing = [k for k in prop_keys if k not in existing_required]
                        if missing:
                            node["required"] = existing_required + missing
                    else:
                        # è‹¥ä¸å­˜åœ¨ required æˆ–æ ¼å¼ä¸å¯¹ï¼Œç›´æ¥è®¾ç½®ä¸ºæ‰€æœ‰å±æ€§é”®
                        node["required"] = prop_keys

            # é€’å½’å¤„ç† properties
            if isinstance(node.get("properties"), dict):
                for k, v in list(node["properties"].items()):
                    node["properties"][k] = _rec(v)

            # patternProperties
            if isinstance(node.get("patternProperties"), dict):
                for k, v in list(node["patternProperties"].items()):
                    node["patternProperties"][k] = _rec(v)

            # itemsï¼ˆæ•°ç»„å…ƒç´ ï¼‰
            it = node.get("items")
            if isinstance(it, dict):
                node["items"] = _rec(it)
            elif isinstance(it, list):
                node["items"] = [_rec(x) for x in it]

            # ç»„åˆå…³é”®å­—
            for comb in ("allOf", "anyOf", "oneOf"):
                if isinstance(node.get(comb), list):
                    node[comb] = [_rec(s) for s in node[comb]]

            # å¦‚æœ additionalProperties æœ¬èº«æ˜¯ schemaï¼Œåˆ™é€’å½’
            ap = node.get("additionalProperties")
            if isinstance(ap, dict):
                node["additionalProperties"] = _rec(ap)

            return node

        return _rec(copy.deepcopy(schema))


class ModelClient:
    """OpenAI å…¼å®¹æ¨¡å‹å®¢æˆ·ç«¯"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æ¨¡å‹å®¢æˆ·ç«¯
        
        Args:
            config: æ¨¡å‹é…ç½®å­—å…¸
        """
        if not OPENAI_AVAILABLE:
            raise ImportError("è¯·å®‰è£…: pip install openai")
        
        self.config = config
        self.provider = config.get('provider', 'openai')
        self.model = config.get('model', 'gpt-4o')
        self.api_key = self._get_api_key_from_env()
        self.base_url = self._get_base_url_from_env()
        self.temperature = config.get('temperature', 0.3)
        self.max_tokens = config.get('max_tokens', 4096)
        self.timeout = config.get('timeout', 120)
        self.supports_vision = config.get('supports_vision', False)
        
        if not self.api_key:
            raise ValueError(f"æœªæ‰¾åˆ° API Keyï¼Œè¯·é…ç½®æˆ–è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY æˆ– DMXAPI_KEY")
        
        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        client_kwargs = {'api_key': self.api_key}
        if self.base_url:
            client_kwargs['base_url'] = self.base_url
        if self.timeout:
            client_kwargs['timeout'] = self.timeout
        
        self.client = OpenAI(**client_kwargs)
        logger.debug(f"{self.provider.upper()} å®¢æˆ·ç«¯åˆå§‹åŒ–: {self.model}")
        
    
    def _get_api_key_from_env(self) -> Optional[str]:
        """ä» .env ç¯å¢ƒå˜é‡è·å– API Keyï¼ˆå”¯ä¸€å…¥å£ï¼‰"""
        api_key = os.environ.get('API_KEY')
        if api_key:
            return api_key
    
    def _get_base_url_from_env(self) -> Optional[str]:
        """ä» .env ç¯å¢ƒå˜é‡è·å– Base URL"""
        # ä¼˜å…ˆçº§1: é€šç”¨é…ç½®
        base_url = os.environ.get('API_BASE_URL')
        if base_url:
            return base_url
        
        # ä¼˜å…ˆçº§2: å…¼å®¹æ—§é…ç½®
        return os.environ.get('OPENAI_BASE_URL') or self.config.get('base_url')
    
    def chat_completion(
        self,
        messages: List[Dict[str, Any]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        json_schema: Optional[Dict] = None,
        use_strict_mode: bool = True,  # â­ æ–°å¢å‚æ•°
        **kwargs
    ) -> Dict[str, Any]:
        """
        èŠå¤©è¡¥å…¨æ¥å£
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦
            max_tokens: æœ€å¤§tokenæ•°
            json_schema: JSON Schemaï¼ˆç”¨äºç»“æ„åŒ–è¾“å‡ºï¼‰
            use_strict_mode: æ˜¯å¦ä½¿ç”¨ä¸¥æ ¼æ¨¡å¼ï¼ˆä»…å½“ json_schema å­˜åœ¨æ—¶æœ‰æ•ˆï¼‰
            
        Returns:
            å“åº”å­—å…¸
        """
        request_params = {
            "model": self.model,
            "messages": messages,
            "max_tokens": max_tokens or self.max_tokens,
            "temperature": temperature if temperature is not None else self.temperature
        }
        
        # â­ å…³é”®æ”¹è¿›ï¼šæ”¯æŒ Strict JSON Schema
        if json_schema:
            if use_strict_mode:
                # Strict Mode: æ¨¡å‹å¿…é¡» 100% éµå®ˆ Schema
                sanitized_schema = _sanitize_json_schema_for_vision(json_schema)
                request_params["response_format"] = {
                    "type": "json_schema",
                    "json_schema": {
                        "name": "structured_output",  # å¯è‡ªå®šä¹‰åç§°
                        "schema": sanitized_schema,
                        "strict": True  # ğŸ”‘ å…³é”®ï¼å¼€å¯ä¸¥æ ¼æ¨¡å¼
                    }
                }
                logger.debug("âœ… å·²å¯ç”¨ Strict JSON Schema Mode")
            else:
                # å…¼å®¹æ¨¡å¼ï¼šä»…è¦æ±‚ JSON æ ¼å¼
                request_params["response_format"] = {"type": "json_object"}
                logger.debug("â„¹ï¸ ä½¿ç”¨å…¼å®¹ JSON æ¨¡å¼ï¼ˆéä¸¥æ ¼ï¼‰")
        
        try:
            response = self.client.chat.completions.create(**request_params)
            content = response.choices[0].message.content
            
            # JSON è§£æ
            if json_schema and content:
                try:
                    content = json.loads(content)
                    logger.debug("âœ… JSON è§£ææˆåŠŸ")
                except json.JSONDecodeError as e:
                    logger.warning(f"âš ï¸ JSON è§£æå¤±è´¥: {str(e)[:100]}ï¼Œè¿”å›åŸå§‹æ–‡æœ¬")
            
            return {
                "content": content,
                "usage": {
                    "input_tokens": response.usage.prompt_tokens,
                    "output_tokens": response.usage.completion_tokens
                },
                "model": response.model
            }
        
        except Exception as e:
            logger.error(f"API è°ƒç”¨å¤±è´¥: {str(e)}")
            raise
    
    def responses_create(
        self,
        inputs: List[Dict[str, Any]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        json_schema: Optional[Dict] = None,
        use_strict_mode: bool = True,  # â­ æ–°å¢å‚æ•°
        **kwargs
    ) -> Dict[str, Any]:
        """
        OpenAI Responses API æ¥å£é€‚é…å™¨ï¼ˆVision æ”¯æŒï¼‰
        â­ æ–°å¢ï¼šæ”¯æŒ Strict JSON Schema Mode
        
        Args:
            inputs: è¾“å…¥åˆ—è¡¨ï¼ˆåœ¨ Agent3 ä¸­ï¼Œè¿™å®é™…ä¸Šæ˜¯ messages åˆ—è¡¨ï¼‰
            temperature: æ¸©åº¦
            max_tokens: æœ€å¤§tokenæ•°
            json_schema: JSON Schema
            use_strict_mode: æ˜¯å¦ä½¿ç”¨ä¸¥æ ¼æ¨¡å¼
            
        Returns:
            å“åº”å­—å…¸
        """
        request_params = {
            "model": self.model,
            "messages": inputs,  # å°† inputs é‡å‘½åä¸º messages
            "max_tokens": max_tokens or self.max_tokens,
            "temperature": temperature if temperature is not None else self.temperature
        }
        
        # â­ å…³é”®æ”¹è¿›ï¼šæ”¯æŒ Strict JSON Schemaï¼ˆè§†è§‰æ¨¡å‹ï¼‰
        if json_schema:
            if use_strict_mode:
                # Strict Mode: è§†è§‰æ¨¡å‹ä¹Ÿæ”¯æŒ
                sanitized_schema = _sanitize_json_schema_for_vision(json_schema)
                request_params["response_format"] = {
                    "type": "json_schema",
                    "json_schema": {
                        "name": "vision_structured_output",
                        "schema": sanitized_schema,
                        "strict": True  # ğŸ”‘ è§†è§‰æ¨¡å‹çš„ä¸¥æ ¼æ¨¡å¼
                    }
                }
                logger.debug("âœ… å·²å¯ç”¨ Vision Strict JSON Schema Mode")
            else:
                # å…¼å®¹æ¨¡å¼
                request_params["response_format"] = {"type": "json_object"}
                logger.debug("â„¹ï¸ ä½¿ç”¨å…¼å®¹ JSON æ¨¡å¼ï¼ˆéä¸¥æ ¼ï¼‰")
        
        # â­ å¯¹äºè§†è§‰æ¨¡å‹ï¼Œåœ¨ system prompt ä¸­å¼ºè°ƒ JSON è¾“å‡ºï¼ˆåŒé‡ä¿é™©ï¼‰
        if self.supports_vision and json_schema:
            for msg in inputs:
                if msg.get("role") == "system":
                    original_content = msg["content"]
                    msg["content"] = (
                        "**CRITICAL: You must respond with ONLY valid JSON. "
                        "No markdown, no explanations, no code blocks. "
                        "Just pure JSON starting with { and ending with }.**\n\n"
                        + original_content
                    )
                    break
        
        try:
            logger.debug(f"è°ƒç”¨ Chat Completions (Vision): model={self.model}, messages={len(inputs)} æ¡")
            
            # ä½¿ç”¨æ ‡å‡†çš„ chat.completions.create æ›¿ä»£ responses.create
            response = self.client.chat.completions.create(**request_params)
            
            # é€‚é…è¿”å›æ ¼å¼
            content = response.choices[0].message.content
            
            # JSON è§£æé€»è¾‘
            if json_schema and content:
                try:
                    # æ¸…ç†å¯èƒ½çš„ Markdown æ ‡è®°
                    import re
                    json_match = re.search(r'```json\s*(.*?)\s*```', content, re.DOTALL)
                    if json_match:
                        content = json.loads(json_match.group(1))
                    else:
                        content = json.loads(content)
                    logger.debug("âœ… JSON è§£ææˆåŠŸ")
                except json.JSONDecodeError as e:
                    logger.warning(f"âš ï¸ JSON è§£æå¤±è´¥: {str(e)[:100]}ï¼Œè¿”å›åŸå§‹æ–‡æœ¬")
            
            return {
                "content": content,
                "usage": {
                    "input_tokens": response.usage.prompt_tokens,
                    "output_tokens": response.usage.completion_tokens
                },
                "model": response.model
            }
        
        except Exception as e:
            logger.error(f"Vision API è°ƒç”¨å¤±è´¥: {str(e)}")
            raise


class ModelClientManager:
    """
    å¤šæ¨¡å‹å®¢æˆ·ç«¯ç®¡ç†å™¨
    æ”¯æŒä¸ºä¸åŒ Agent é…ç½®ä¸åŒçš„æ¨¡å‹
    """
    
    def __init__(self, config_path: str = "config/model_config.yaml"):
        """
        åˆå§‹åŒ–ç®¡ç†å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        import yaml
        from pathlib import Path
        
        # åŠ è½½ YAML é…ç½®
        config_file = Path(config_path)
        if not config_file.exists():
            raise FileNotFoundError(f"æ¨¡å‹é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        
        with open(config_file, 'r', encoding='utf-8') as f:
            self.full_config = yaml.safe_load(f)
        
        self.default_config = self.full_config.get('default', {})
        self.agents_config = self.full_config.get('agents', {})
        self._clients_cache = {}  # Agentå®¢æˆ·ç«¯ç¼“å­˜
        
        logger.info(f"æ¨¡å‹å®¢æˆ·ç«¯ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"é»˜è®¤æ¨¡å‹: {self.default_config.get('provider')}/{self.default_config.get('model')}")
        logger.info(f"å·²é…ç½® {len(self.agents_config)} ä¸ªAgentæ¨¡å‹")
    
    def _merge_config(self, agent_config: Dict, default_config: Dict) -> Dict:
        """åˆå¹¶ Agent é…ç½®å’Œé»˜è®¤é…ç½®"""
        merged = default_config.copy()
        merged.update(agent_config)
        return merged
    
    def get_client(self, agent_name: str = "default") -> ModelClient:
        """
        è·å–æŒ‡å®š Agent çš„å®¢æˆ·ç«¯
        
        Args:
            agent_name: Agentåç§°
            
        Returns:
            æ¨¡å‹å®¢æˆ·ç«¯å®ä¾‹
        """
        # æ£€æŸ¥ç¼“å­˜
        if agent_name in self._clients_cache:
            return self._clients_cache[agent_name]
        
        # è·å– Agent é…ç½®
        if agent_name in self.agents_config:
            agent_config = self.agents_config[agent_name]
            full_config = self._merge_config(agent_config, self.default_config)
        else:
            full_config = self.default_config
        
        # åˆ›å»ºå®¢æˆ·ç«¯
        client = ModelClient(full_config)
        
        # ç¼“å­˜
        self._clients_cache[agent_name] = client
        
        logger.info(
            f"ä¸º [{agent_name}] åˆ›å»ºå®¢æˆ·ç«¯: "
            f"{full_config.get('provider')}/{full_config.get('model')}"
        )
        
        return client
    
    def chat_completion(
        self,
        messages: List[Dict[str, Any]],
        agent_name: str = "default",
        json_schema: Optional[Dict] = None,
        use_strict_mode: bool = True,  # â­ æ–°å¢å‚æ•°
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€çš„èŠå¤©è¡¥å…¨æ¥å£
        â­ æ–°å¢ï¼šæ”¯æŒ Strict JSON Schema Mode
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            agent_name: Agentåç§°
            json_schema: JSON Schema
            use_strict_mode: æ˜¯å¦ä½¿ç”¨ä¸¥æ ¼æ¨¡å¼ï¼ˆé»˜è®¤ Trueï¼‰
            temperature: æ¸©åº¦
            max_tokens: æœ€å¤§tokenæ•°
            
        Returns:
            å“åº”å­—å…¸
        """
        client = self.get_client(agent_name)
        
        logger.info(f"[{agent_name}] è°ƒç”¨æ¨¡å‹: {client.provider}/{client.model}")
        
        if json_schema and use_strict_mode:
            logger.info(f"[{agent_name}] ğŸ”’ å¯ç”¨ Strict JSON Schema Mode")
        
        result = client.chat_completion(
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            json_schema=json_schema,
            use_strict_mode=use_strict_mode,
            **kwargs
        )
        
        # æ·»åŠ  Agent ä¿¡æ¯
        result['agent_name'] = agent_name
        result['provider'] = client.provider
        
        logger.success(
            f"[{agent_name}] âœ“ å®Œæˆ "
            f"(è¾“å…¥:{result['usage']['input_tokens']} "
            f"è¾“å‡º:{result['usage']['output_tokens']})"
        )
        
        return result
    
    def responses_create(
        self,
        inputs: List[Dict[str, Any]],
        agent_name: str = "agent3",
        json_schema: Optional[Dict] = None,
        use_strict_mode: bool = True,  # â­ æ–°å¢å‚æ•°
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        OpenAI Responses API æ¥å£ï¼ˆç”¨äº Agent3 å¤šå›¾ç‰‡è¾“å…¥ï¼‰
        â­ æ–°å¢ï¼šæ”¯æŒ Strict JSON Schema Mode
        
        Args:
            inputs: è¾“å…¥åˆ—è¡¨
            agent_name: Agentåç§°
            json_schema: JSON Schema
            use_strict_mode: æ˜¯å¦ä½¿ç”¨ä¸¥æ ¼æ¨¡å¼ï¼ˆé»˜è®¤ Trueï¼‰
            temperature: æ¸©åº¦
            max_tokens: æœ€å¤§tokenæ•°
            
        Returns:
            å“åº”å­—å…¸
        """
        client = self.get_client(agent_name)
        
        logger.info(f"[{agent_name}] è°ƒç”¨ Responses API: {client.provider}/{client.model}")
        
        if json_schema and use_strict_mode:
            logger.info(f"[{agent_name}] ğŸ”’ å¯ç”¨ Vision Strict JSON Schema Mode")
        
        result = client.responses_create(
            inputs=inputs,
            temperature=temperature,
            max_tokens=max_tokens,
            json_schema=json_schema,
            use_strict_mode=use_strict_mode,
            **kwargs
        )
        
        # æ·»åŠ  Agent ä¿¡æ¯
        result['agent_name'] = agent_name
        result['provider'] = client.provider
        
        logger.success(
            f"[{agent_name}] âœ“ Responses API å®Œæˆ "
            f"(è¾“å…¥:{result['usage']['input_tokens']} "
            f"è¾“å‡º:{result['usage']['output_tokens']})"
        )
        
        return result
    
    def get_model_info(self, agent_name: str = "default") -> Dict[str, Any]:
        """è·å–æŒ‡å®š Agent çš„æ¨¡å‹ä¿¡æ¯"""
        client = self.get_client(agent_name)
        return {
            "agent_name": agent_name,
            "provider": client.provider,
            "model": client.model,
            "supports_vision": client.supports_vision,
            "temperature": client.temperature,
            "max_tokens": client.max_tokens
        }
    
    def list_all_agents(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰é…ç½®çš„ Agent"""
        return list(self.agents_config.keys())


# å·¥å‚å‡½æ•°ï¼ˆå‘åå…¼å®¹ï¼‰
class ModelClientFactory:
    """æ¨¡å‹å®¢æˆ·ç«¯å·¥å‚"""
    
    @staticmethod
    def create_from_config(config_path: str = "config/model_config.yaml") -> ModelClientManager:
        """
        ä»é…ç½®æ–‡ä»¶åˆ›å»ºç®¡ç†å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            
        Returns:
            ModelClientManager å®ä¾‹
        """
        return ModelClientManager(config_path)