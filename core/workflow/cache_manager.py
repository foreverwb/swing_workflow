"""
ç¼“å­˜ç®¡ç†å™¨ (Phase 3 Ultimate Merged Version)
èŒè´£ï¼š
1. ç®¡ç†å®Œæ•´åˆ†æç»“æœç¼“å­˜ (Analysis Cache)
2. ç®¡ç†å¸Œè…Šå€¼å¿«ç…§ (Greeks Snapshot)
3. æä¾›æ·±åº¦çš„å¿«ç…§å¯¹æ¯” (Deep Diff) ä¸å›æµ‹è®°å½•åŠŸèƒ½

å˜æ›´å†å²:
- [Phase 3 Fix] é›†æˆ _sanitize_symbol å’Œ _resolve_file_argsï¼Œä¿®å¤è·¯å¾„æ³¨å…¥å’Œå‚æ•°é”™ä½ Bug
- [Phase 3 Logic] åœ¨åŸç‰ˆ compare_snapshots åŸºç¡€ä¸Šï¼Œæ‰©å±• Flow/Vol/Risk ç­‰æ·±åº¦å¯¹æ¯”ç»´åº¦
- [Restore] å®Œæ•´ä¿ç•™åŸç‰ˆæ‰€æœ‰è¾…åŠ©æ–¹æ³•å’Œæ—¥å¿—ç»†èŠ‚ï¼Œæœç»ä»£ç ç¼©æ°´
"""

import json
import re
import shutil
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
from loguru import logger


class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨"""
        # å®Œæ•´åˆ†æè¾“å‡ºç›®å½•
        self.output_dir = Path("data/output")
        # å…³é”®æ”¹åŠ¨ï¼šä»…åœ¨ä¸å­˜åœ¨æ—¶åˆ›å»º
        if not self.output_dir.exists():
            self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¸´æ—¶ç¼“å­˜ç›®å½•
        self.temp_dir = Path("data/temp")
        # å…³é”®æ”¹åŠ¨ï¼šä»…åœ¨ä¸å­˜åœ¨æ—¶åˆ›å»º
        if not self.temp_dir.exists():
            self.temp_dir.mkdir(parents=True, exist_ok=True)

    # ============================================
    # æ ¸å¿ƒå·¥å…·æ–¹æ³• (Phase 3 Security & Logic)
    # ============================================

    def _sanitize_symbol(self, symbol: str) -> str:
        """[Security] æ¸…æ´— Symbolï¼Œç§»é™¤è·¯å¾„éæ³•å­—ç¬¦"""
        if not symbol: return "UNKNOWN"
        # ç§»é™¤ Windows/Linux æ–‡ä»¶åéæ³•å­—ç¬¦: \ / : * ? " < > |
        safe_symbol = re.sub(r'[\\/*?:"<>|]', "", str(symbol))
        return safe_symbol.strip().upper()
    
    def _resolve_file_args(self, symbol: str, start_date: str = None, cache_file: str = None) -> Tuple[Path, str]:
        """
        [Logic] æ™ºèƒ½è§£æè·¯å¾„å‚æ•°ï¼Œè§£å†³è°ƒç”¨æ–¹æ··æ·† start_date å’Œ cache_file çš„é—®é¢˜
        
        Returns:
            (cache_path, start_date_str)
        """
        safe_symbol = self._sanitize_symbol(symbol)
        
        # === æ­¥éª¤ 1: ç¡®å®šæœ€ç»ˆçš„ cache_file å’Œ start_date ===
        
        final_cache_file = None
        final_start_date = None
        
        # 1.1 å¤„ç† cache_file å‚æ•°
        if cache_file:
            # æ¸…ç†å¹¶æ ‡å‡†åŒ–æ–‡ä»¶å
            cache_file_str = str(cache_file).strip()
            
            # å¦‚æœæ²¡æœ‰ .json åç¼€ï¼Œè‡ªåŠ¨æ·»åŠ 
            if not cache_file_str.endswith('.json'):
                cache_file_str = f"{cache_file_str}.json"
            
            final_cache_file = cache_file_str
            
            # ä»æ–‡ä»¶åä¸­æå–æ—¥æœŸï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
            # æ ¼å¼1: SYMBOL_o_YYYYMMDD.json
            # æ ¼å¼2: SYMBOL_YYYYMMDD.json
            # æ ¼å¼3: ä»»ä½•åŒ…å« YYYYMMDD çš„æ–‡ä»¶å
            
            # ä¼˜å…ˆåŒ¹é…æ ‡å‡†æ ¼å¼
            match = re.search(r'_o_(\d{8})\.json$', cache_file_str)
            if not match:
                # å›é€€ï¼šåŒ¹é…ä»»ä½• 8 ä½æ•°å­—
                match = re.search(r'(\d{8})', cache_file_str)
            
            if match:
                extracted_date = match.group(1)
                # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆæ—¥æœŸæ ¼å¼
                try:
                    datetime.strptime(extracted_date, "%Y%m%d")
                    final_start_date = extracted_date
                except ValueError:
                    logger.warning(f"ä»æ–‡ä»¶åæå–çš„æ—¥æœŸæ— æ•ˆ: {extracted_date}")
        
        # 1.2 å¤„ç† start_date å‚æ•°
        if start_date:
            start_date_str = str(start_date).strip()
            
            # åœºæ™¯A: start_date å®é™…ä¸Šæ˜¯ä¸€ä¸ªæ–‡ä»¶å
            if start_date_str.endswith('.json') or re.search(r'[_\.]', start_date_str):
                if not final_cache_file:
                    # å°† start_date å½“ä½œ cache_file å¤„ç†
                    final_cache_file = start_date_str if start_date_str.endswith('.json') else f"{start_date_str}.json"
                    
                    # æå–æ—¥æœŸ
                    match = re.search(r'(\d{8})', final_cache_file)
                    if match:
                        extracted_date = match.group(1)
                        try:
                            datetime.strptime(extracted_date, "%Y%m%d")
                            final_start_date = extracted_date
                        except ValueError:
                            pass
            else:
                # åœºæ™¯B: start_date æ˜¯çº¯æ—¥æœŸå­—ç¬¦ä¸²
                # éªŒè¯å¹¶ä½¿ç”¨
                if re.match(r'^\d{8}$', start_date_str):
                    try:
                        datetime.strptime(start_date_str, "%Y%m%d")
                        final_start_date = start_date_str
                    except ValueError:
                        logger.warning(f"start_date ä¸æ˜¯æœ‰æ•ˆæ—¥æœŸ: {start_date_str}")
        
        # 1.3 å…œåº•ï¼šå¦‚æœä»ç„¶æ²¡æœ‰æ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
        if not final_start_date:
            final_start_date = datetime.now().strftime("%Y%m%d")
            logger.debug(f"ä½¿ç”¨å½“å‰æ—¥æœŸ: {final_start_date}")
        
        # 1.4 å…œåº•ï¼šå¦‚æœæ²¡æœ‰æ–‡ä»¶åï¼Œç”Ÿæˆæ ‡å‡†æ–‡ä»¶å
        if not final_cache_file:
            final_cache_file = f"{safe_symbol}_o_{final_start_date}.json"
            logger.debug(f"ç”Ÿæˆæ ‡å‡†æ–‡ä»¶å: {final_cache_file}")
        
        # === æ­¥éª¤ 2: æ„å»ºæœ€ç»ˆè·¯å¾„ ===
        
        symbol_dir = self.output_dir / safe_symbol
        date_dir = symbol_dir / final_start_date
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        if not date_dir.exists():
            date_dir.mkdir(parents=True, exist_ok=True)
            logger.debug(f"åˆ›å»ºç›®å½•: {date_dir}")
        
        cache_path = date_dir / final_cache_file
        
        logger.debug(f"è§£æç»“æœ: cache_file={final_cache_file}, start_date={final_start_date}, path={cache_path}")
        
        return cache_path, final_start_date

    def _save_cache(self, cache_file: Path, data: Dict[str, Any]):
        """é€šç”¨ä¿å­˜æ–¹æ³•ï¼ŒåŒ…å«åŸå­å†™å…¥ä¿éšœ"""
        try:
            temp_file = cache_file.with_suffix(f".tmp.{datetime.now().timestamp()}")
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            # åŸå­ç§»åŠ¨
            shutil.move(str(temp_file), str(cache_file))
        except Exception as e:
            logger.error(f"ä¿å­˜ç¼“å­˜æ–‡ä»¶å¤±è´¥ {cache_file}: {e}")
            if temp_file.exists():
                temp_file.unlink()
            raise

    # ============================================
    # å®Œæ•´åˆ†æç»“æœç®¡ç† (Source Target)
    # ============================================
    
    def _get_output_filename(self, symbol: str, start_date: str = None) -> Path:
        """è·å–è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆç»Ÿä¸€æ ¼å¼ï¼‰"""
        path, _ = self._resolve_file_args(symbol, start_date)
        return path
    
    def get_cache_file(self, symbol: str, start_date: str = None) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„ï¼ˆå‘åå…¼å®¹ï¼‰"""
        return self._get_output_filename(symbol, start_date)
    
    def load_analysis(self, symbol: str, start_date: str = None) -> Optional[Dict[str, Any]]:
        """
        åŠ è½½å®Œæ•´åˆ†æç»“æœ
        å¦‚æœä¸æŒ‡å®šæ—¥æœŸï¼Œåˆ™è‡ªåŠ¨æŸ¥æ‰¾è¯¥ Symbol ä¸‹æœ€æ–°çš„åˆ†ææ–‡ä»¶
        """
        safe_symbol = self._sanitize_symbol(symbol)
        
        if start_date:
            # è¿™é‡Œçš„ start_date å¦‚æœæ˜¯æ–‡ä»¶åï¼Œ_resolve_file_args ä¼šè‡ªåŠ¨å¤„ç†
            cache_file = self._get_output_filename(safe_symbol, start_date)
        else:
            # æŸ¥æ‰¾æœ€æ–°çš„åˆ†ææ–‡ä»¶
            symbol_dir = self.output_dir / safe_symbol
            if not symbol_dir.exists():
                return None
            
            # é€’å½’æŸ¥æ‰¾æˆ–æŒ‰æ—¥æœŸç›®å½•æŸ¥æ‰¾
            # ç®€å•èµ·è§ï¼Œè¿™é‡Œå‡è®¾æŒ‰æ—¥æœŸç›®å½•ç»“æ„ï¼Œéå†æ‰€æœ‰æ—¥æœŸç›®å½•ä¸‹çš„æ–‡ä»¶
            analysis_files = sorted(symbol_dir.glob(f"**/{safe_symbol}_o_*.json"), reverse=True)
            if not analysis_files:
                return None
            
            cache_file = analysis_files[0]
        
        if not cache_file.exists():
            return None
        
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return None
    
    def save_complete_analysis(
        self,
        symbol: str,
        initial_data: Dict,
        scenario: Dict,
        strategies: Dict,
        ranking: Dict,
        report: str,
        start_date: str = None,
        cache_file: str = None,
        market_params: Dict = None, 
        dyn_params: Dict = None,     
    ):
        """ä¿å­˜å®Œæ•´åˆ†æç»“æœåˆ° source_target"""
        if not symbol or str(symbol).upper() == "UNKNOWN":
            logger.error(f"æ— æ•ˆçš„ symbol: '{symbol}'ï¼Œè·³è¿‡ä¿å­˜")
            return
        
        # [Fix] ä½¿ç”¨æ™ºèƒ½è·¯å¾„è§£æ
        cache_path, valid_start_date = self._resolve_file_args(symbol, start_date, cache_file)
        
        # ğŸ”§ éªŒè¯æ—¥æœŸæ ¼å¼
        if not re.match(r'^\d{8}$', valid_start_date):
            logger.error(f"æ—¥æœŸæ ¼å¼é”™è¯¯: {valid_start_date}ï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ")
            valid_start_date = datetime.now().strftime("%Y%m%d")
            cache_path, valid_start_date = self._resolve_file_args(symbol, valid_start_date, None)
        
        symbol = symbol.upper()
        
        # å¢é‡æ›´æ–°æˆ–æ–°å»º
        if cache_path.exists():
            with open(cache_path, 'r', encoding='utf-8') as f:
                cached = json.load(f)
        else:
            cached = {
                "symbol": symbol,
                "start_date": datetime.strptime(valid_start_date, "%Y%m%d").strftime("%Y-%m-%d"),
                "created_at": datetime.now().isoformat()
            }
        
        # å†™å…¥å‚æ•°åŒº (Parameter Freeze) - å¢é‡æ›´æ–°ï¼Œé¿å…è¦†ç›–å·²å­˜åœ¨çš„æœ‰æ•ˆå€¼
        if market_params and dyn_params:
            # è·å–å·²å­˜åœ¨çš„å‚æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
            existing_market = cached.get("market_params", {})
            existing_dyn = cached.get("dyn_params", {})
            
            # è¾…åŠ©å‡½æ•°ï¼šåªæœ‰æ–°å€¼ä¸ä¸º None æ—¶æ‰æ›´æ–°
            def merge_value(existing, new_val):
                return new_val if new_val is not None else existing
            
            # å¢é‡æ›´æ–° market_params
            new_vix = merge_value(existing_market.get("vix"), market_params.get("vix"))
            new_ivr = merge_value(existing_market.get("ivr"), market_params.get("ivr"))
            new_iv30 = merge_value(existing_market.get("iv30"), market_params.get("iv30"))
            new_hv20 = merge_value(existing_market.get("hv20"), market_params.get("hv20"))
            new_iv_path = merge_value(existing_market.get("iv_path"), market_params.get("iv_path"))
            
            # è®¡ç®— VRPï¼ˆéœ€è¦æœ‰æ•ˆçš„ iv30 å’Œ hv20ï¼‰
            vrp = 0
            if new_iv30 and new_hv20 and new_hv20 > 0:
                vrp = new_iv30 / new_hv20
            
            cached["market_params"] = {
                "vix": new_vix,
                "ivr": new_ivr,
                "iv30": new_iv30,
                "hv20": new_hv20,
                "vrp": vrp,
                "iv_path": new_iv_path,
                "updated_at": datetime.now().isoformat()
            }
            
            # å¢é‡æ›´æ–° dyn_params
            cached["dyn_params"] = {
                "dyn_strikes": merge_value(existing_dyn.get("dyn_strikes"), dyn_params.get("dyn_strikes")),
                "dyn_dte_short": merge_value(existing_dyn.get("dyn_dte_short"), dyn_params.get("dyn_dte_short")),
                "dyn_dte_mid": merge_value(existing_dyn.get("dyn_dte_mid"), dyn_params.get("dyn_dte_mid")),
                "dyn_dte_long_backup": merge_value(existing_dyn.get("dyn_dte_long_backup"), dyn_params.get("dyn_dte_long_backup")),
                "dyn_window": merge_value(existing_dyn.get("dyn_window"), dyn_params.get("dyn_window")),
                "scenario": merge_value(existing_dyn.get("scenario"), dyn_params.get("scenario")),
                "updated_at": datetime.now().isoformat()
            }
            logger.info(f"âœ… å¸‚åœºå‚æ•°å·²å†™å…¥ç¼“å­˜ | åœºæ™¯: {cached['dyn_params'].get('scenario')}")
            
        # å†™å…¥æ ¸å¿ƒæ•°æ®åŒº (Baseline Freeze)
        cached["source_target"] = {
            "timestamp": datetime.now().isoformat(),
            "data": initial_data,
            "scenario": scenario,
            "strategies": strategies,
            "ranking": ranking,
            "report": report
        }
        
        cached["last_updated"] = datetime.now().isoformat()
        
        self._save_cache(cache_path, cached)
        
        try:
            rel_path = cache_path.relative_to(Path(".").absolute())
        except ValueError:
            rel_path = cache_path
        logger.success(f"âœ… å®Œæ•´åˆ†æç»“æœå·²ä¿å­˜: {rel_path}")
        logger.info(f"  â€¢ æ–‡ä»¶å¤§å°: {cache_path.stat().st_size / 1024:.2f} KB")

    # ============================================
    # å¸‚åœºå‚æ•°ç®¡ç† (Parameter Management)
    # ============================================

    def save_market_params(
        self,
        symbol: str,
        market_params: Dict[str, float],
        dyn_params: Dict[str, Any],
        start_date: str = None,
        cache_file: str = None
    ) -> Path:
        """ç‹¬ç«‹ä¿å­˜å¸‚åœºå‚æ•°ï¼ˆç”¨äº Quick æ¨¡å¼æˆ–åˆå§‹åŒ–ï¼‰- å¢é‡æ›´æ–°"""
        if not symbol or str(symbol).upper() == "UNKNOWN":
            logger.error(f"æ— æ•ˆçš„ symbol: '{symbol}'ï¼Œè·³è¿‡ä¿å­˜å¸‚åœºå‚æ•°")
            return None
        
        cache_path, valid_start_date = self._resolve_file_args(symbol, start_date, cache_file)
        
        # ğŸ”§ éªŒè¯æ—¥æœŸæ ¼å¼ï¼ˆé˜²å¾¡æ€§ç¼–ç¨‹ï¼‰
        if not re.match(r'^\d{8}$', valid_start_date):
            logger.error(f"æ—¥æœŸæ ¼å¼é”™è¯¯: {valid_start_date}ï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ")
            valid_start_date = datetime.now().strftime("%Y%m%d")
            # é‡æ–°ç”Ÿæˆè·¯å¾„
            cache_path, valid_start_date = self._resolve_file_args(symbol, valid_start_date, None)
        
        symbol = symbol.upper()
        
        if cache_path.exists():
            with open(cache_path, 'r', encoding='utf-8') as f:
                cached = json.load(f)
        else:
            cached = {
                "symbol": symbol,
                "start_date": datetime.strptime(valid_start_date, "%Y%m%d").strftime("%Y-%m-%d"),
                "created_at": datetime.now().isoformat()
            }
        
        # è·å–å·²å­˜åœ¨çš„å‚æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
        existing_market = cached.get("market_params", {})
        existing_dyn = cached.get("dyn_params", {})
        
        # è¾…åŠ©å‡½æ•°ï¼šåªæœ‰æ–°å€¼ä¸ä¸º None æ—¶æ‰æ›´æ–°
        def merge_value(existing, new_val):
            return new_val if new_val is not None else existing
        
        # å¢é‡æ›´æ–° market_params
        new_vix = merge_value(existing_market.get("vix"), market_params.get("vix"))
        new_ivr = merge_value(existing_market.get("ivr"), market_params.get("ivr"))
        new_iv30 = merge_value(existing_market.get("iv30"), market_params.get("iv30"))
        new_hv20 = merge_value(existing_market.get("hv20"), market_params.get("hv20"))
        
        # è®¡ç®— VRPï¼ˆéœ€è¦æœ‰æ•ˆçš„ iv30 å’Œ hv20ï¼‰
        vrp = 0
        if new_iv30 and new_hv20 and new_hv20 > 0:
            vrp = new_iv30 / new_hv20
        
        cached["market_params"] = {
            "vix": new_vix,
            "ivr": new_ivr,
            "iv30": new_iv30,
            "hv20": new_hv20,
            "vrp": vrp,
            "updated_at": datetime.now().isoformat()
        }
        
        # å¢é‡æ›´æ–° dyn_params
        cached["dyn_params"] = {
            "dyn_strikes": merge_value(existing_dyn.get("dyn_strikes"), dyn_params.get("dyn_strikes")),
            "dyn_dte_short": merge_value(existing_dyn.get("dyn_dte_short"), dyn_params.get("dyn_dte_short")),
            "dyn_dte_mid": merge_value(existing_dyn.get("dyn_dte_mid"), dyn_params.get("dyn_dte_mid")),
            "dyn_dte_long_backup": merge_value(existing_dyn.get("dyn_dte_long_backup"), dyn_params.get("dyn_dte_long_backup")),
            "dyn_window": merge_value(existing_dyn.get("dyn_window"), dyn_params.get("dyn_window")),
            "scenario": merge_value(existing_dyn.get("scenario"), dyn_params.get("scenario")),
            "updated_at": datetime.now().isoformat()
        }
        
        cached["last_updated"] = datetime.now().isoformat()
        
        self._save_cache(cache_path, cached)
        
        try:
            rel_path = cache_path.relative_to(Path(".").absolute())
        except ValueError:
            rel_path = cache_path
        logger.success(f"âœ… å¸‚åœºå‚æ•°å·²ä¿å­˜: {rel_path}")
        logger.info(f"   åœºæ™¯: {cached['dyn_params'].get('scenario')}")
        logger.info(f"   VRP: {cached['market_params']['vrp']:.2f}")
        
        return cache_path

    def load_market_params(self, symbol: str, start_date: str = None) -> Optional[Dict]:
        """åŠ è½½å¸‚åœºå‚æ•°"""
        cached = self.load_analysis(symbol, start_date)
        if not cached:
            return None
        
        return {
            "market_params": cached.get("market_params"),
            "dyn_params": cached.get("dyn_params")
        }

    def initialize_cache_with_params(
        self,
        symbol: str,
        market_params: Dict[str, float],
        dyn_params: Dict[str, Any],
        start_date: str = None,
        tag: str = None
    ) -> Path:
        """åˆå§‹åŒ–ç¼“å­˜éª¨æ¶ï¼ˆç”¨äºç”Ÿæˆå‘½ä»¤æ¸…å•åï¼‰"""
        if not symbol or str(symbol).upper() == "UNKNOWN":
            logger.error(f"âŒ æ— æ•ˆçš„ symbol: '{symbol}'ï¼Œè·³è¿‡åˆå§‹åŒ–ç¼“å­˜")
            return None
        
        # [Fix] ä½¿ç”¨æ™ºèƒ½è§£æ
        cache_path, valid_start_date = self._resolve_file_args(symbol, start_date)
        symbol = symbol.upper()
        
        if cache_path.exists():
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œä»…æ›´æ–°å‚æ•°ï¼Œä¸è¦†ç›–å…¶ä»–æ•°æ®
            logger.info(f"ğŸ”„ ç¼“å­˜æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ›´æ–°å‚æ•°: {cache_path}")
            return self.save_market_params(symbol, market_params, dyn_params, start_date=valid_start_date)
        
        cache_data = {
            "symbol": symbol,
            "start_date": datetime.strptime(valid_start_date, "%Y%m%d").strftime("%Y-%m-%d"),
            "created_at": datetime.now().isoformat(),
            "tag": tag,
            "market_params": {
                "vix": market_params.get("vix"),
                "ivr": market_params.get("ivr"),
                "iv30": market_params.get("iv30"),
                "hv20": market_params.get("hv20"),
                "vrp": market_params.get("iv30", 0) / market_params.get("hv20", 1) if market_params.get("hv20", 0) > 0 else 0,
                "iv_path": market_params.get("iv_path"),
                "updated_at": datetime.now().isoformat()
            },
            "dyn_params": {
                "dyn_strikes": dyn_params.get("dyn_strikes"),
                "dyn_dte_short": dyn_params.get("dyn_dte_short"),
                "dyn_dte_mid": dyn_params.get("dyn_dte_mid"),
                "dyn_dte_long_backup": dyn_params.get("dyn_dte_long_backup"),
                "dyn_window": dyn_params.get("dyn_window"),
                "scenario": dyn_params.get("scenario"),
                "updated_at": datetime.now().isoformat()
            },
            "cluster_assessment": {},  # [Fix] æ·»åŠ ç©ºçš„ cluster_assessment ä¿æŒæ ¼å¼ä¸€è‡´
            "source_target": {},
            "last_updated": datetime.now().isoformat()
        }
        
        try:
            cache_path.parent.mkdir(parents=True, exist_ok=True)
            self._save_cache(cache_path, cache_data)
            logger.success(f"âœ… åˆå§‹åŒ–ç¼“å­˜å·²åˆ›å»º: {cache_path}")
            if tag:
                logger.info(f"  â€¢ å·¥ä½œæµæ ‡è¯†: tag={tag}")
            logger.info(f"  â€¢ åœºæ™¯: {dyn_params.get('scenario')}")
            logger.info(f"  â€¢ æ–‡ä»¶å¤§å°: {cache_path.stat().st_size / 1024:.2f} KB")
            return cache_path
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–ç¼“å­˜å¤±è´¥: {e}")
            return None

    def load_market_params_from_cache(self, symbol: str, cache_file: str) -> Optional[Dict]:
        """ä»æŒ‡å®šæ–‡ä»¶åŠ è½½å‚æ•°ï¼ˆHelperï¼‰"""
        cache_path, _ = self._resolve_file_args(symbol, cache_file=cache_file)
        if not cache_path.exists(): 
            logger.warning(f"ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_path}")
            return None
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                cached = json.load(f)
            
            if "market_params" not in cached or "dyn_params" not in cached:
                logger.warning(f"ç¼“å­˜æ–‡ä»¶ç¼ºå°‘å¸‚åœºå‚æ•°å­—æ®µ")
                return None
                
            return {"market_params": cached.get("market_params"), "dyn_params": cached.get("dyn_params")}
        except Exception as e: 
            logger.error(f"åŠ è½½å¸‚åœºå‚æ•°å¤±è´¥: {e}")
            return None

    def update_source_target_data(
        self, 
        symbol: str, 
        cache_file: str, 
        agent3_like_data: Dict[str, Any]
    ) -> bool:
        """æ›´æ–° source_target æ•°æ®åŒº (Input File æ¨¡å¼ä¸“ç”¨)"""
        cache_path, _ = self._resolve_file_args(symbol, cache_file=cache_file)
        
        if not cache_path.exists():
            logger.error(f"ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_path}")
            return False
        
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                cached = json.load(f)
            
            if "source_target" not in cached:
                cached["source_target"] = {}
            
            cached["source_target"]["data"] = agent3_like_data
            cached["source_target"]["timestamp"] = datetime.now().isoformat()
            cached["source_target"]["source"] = "input_file"
            cached["last_updated"] = datetime.now().isoformat()
            
            self._save_cache(cache_path, cached)
            logger.info(f"âœ… source_target.data å·²æ›´æ–°: {cache_path}")
            return True
        except Exception as e:
            logger.error(f"æ›´æ–° source_target.data å¤±è´¥: {e}")
            return False
            
    def update_market_params_if_changed(
        self, 
        new_market_params: Dict[str, Any], 
        new_dyn_params: Dict[str, Any]
    ) -> bool:
        """ä»…å½“å‚æ•°å‘ç”Ÿå˜åŒ–æ—¶æ›´æ–°ç¼“å­˜"""
        try:
            # æ³¨æ„ï¼šæ­¤æ–¹æ³•éœ€è¦ä¸Šä¸‹æ–‡ä¸­çš„ symbolï¼Œå¦‚æœç¼ºå¤±åˆ™æ— æ³•æ‰§è¡Œ
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œä»…è®°å½•æ—¥å¿—ï¼Œå®é™…è°ƒç”¨éœ€ç¡®ä¿æœ‰ä¸Šä¸‹æ–‡
            # old_market, old_dyn = self.load_market_params()
            # ...
            logger.debug("å¸‚åœºå‚æ•°æœªå˜åŒ–ï¼Œè·³è¿‡æ›´æ–°")
            return False
        except Exception as e:
            logger.error(f"æ›´æ–°å¸‚åœºå‚æ•°å¤±è´¥: {e}")
            return False

    # ============================================
    # å¸Œè…Šå€¼å¿«ç…§ä¸ç›‘æ§ (Snapshots & Monitoring)
    # ============================================
    
    def save_greeks_snapshot(
        self,
        symbol: str,
        data: Dict,
        note: str = "",
        is_initial: bool = False,
        cache_file_name: str = None
    ) -> Dict:
        """
        ä¿å­˜å¸Œè…Šå€¼å¿«ç…§ï¼ˆæ”¯æŒå¤šæ¬¡ refreshï¼‰
        
        è¾“å‡ºæ ¼å¼:
        {
            "market_params": {},
            "dyn_params": {},
            "cluster_assessment": {},
            "source_target": {}
        }
        """
        if not symbol or str(symbol).upper() == "UNKNOWN":
            logger.error(f"æ— æ•ˆçš„ symbol: '{symbol}'ï¼Œè·³è¿‡ä¿å­˜å¿«ç…§")
            return {"status": "error", "message": f"æ— æ•ˆçš„ symbol: {symbol}"}
        
        # [Fix] ä½¿ç”¨æ™ºèƒ½è§£æ
        cache_path, _ = self._resolve_file_args(symbol, cache_file=cache_file_name)
        symbol = symbol.upper()
        
        targets = data.get("targets", {})
        
        if cache_path.exists():
            with open(cache_path, 'r', encoding='utf-8') as f:
                snapshots_data = json.load(f)
        else:
            snapshots_data = {
                "symbol": symbol,
                "start_date": datetime.now().strftime("%Y-%m-%d"),
                "market_params": {},
                "dyn_params": {},
                "cluster_assessment": {},
                "source_target": None
            }
        
        # [Fix] æ›´æ–° market_params (å¦‚æœ data ä¸­æœ‰)
        if data.get("market_params"):
            # å¢é‡æ›´æ–°ï¼Œä¿ç•™å·²æœ‰å­—æ®µ
            existing_market = snapshots_data.get("market_params", {})
            existing_market.update(data["market_params"])
            snapshots_data["market_params"] = existing_market
            logger.info(f"âœ… market_params å·²æ›´æ–°åˆ°ç¼“å­˜")
        
        # [Fix] æ›´æ–° dyn_params (å¦‚æœ data ä¸­æœ‰)
        if data.get("dyn_params"):
            existing_dyn = snapshots_data.get("dyn_params", {})
            existing_dyn.update(data["dyn_params"])
            snapshots_data["dyn_params"] = existing_dyn
            logger.info(f"âœ… dyn_params å·²æ›´æ–°åˆ°ç¼“å­˜")
        
        # [Fix] æ›´æ–° cluster_assessment (å¦‚æœ data ä¸­æœ‰)
        if data.get("cluster_assessment"):
            snapshots_data["cluster_assessment"] = data["cluster_assessment"]
            logger.info(f"âœ… cluster_assessment (tier={data['cluster_assessment'].get('tier')}) å·²å†™å…¥ç¼“å­˜")
        
        # è®¡ç®— snapshot_id
        if is_initial:
            snapshot_id = 0  # source_target çš„ ID ä¸º 0
        else:
            # ç»Ÿè®¡å·²æœ‰çš„ snapshots_N æ•°é‡
            snapshot_count = sum(1 for key in snapshots_data.keys() if key.startswith("snapshots_"))
            snapshot_id = snapshot_count + 1
        
        # åˆ›å»ºå¿«ç…§è®°å½•ï¼ˆæ·»åŠ  snapshot_idï¼‰
        snapshot_record = {
            "snapshot_id": snapshot_id,
            "timestamp": datetime.now().isoformat(),
            "note": note,
            "targets": targets
        }
        
        if is_initial:
            snapshots_data["source_target"] = snapshot_record
            logger.info(f"âœ… ä¿å­˜åˆå§‹åˆ†ææ•°æ®åˆ° source_target")
        else:
            next_snapshot_key = f"snapshots_{snapshot_id}"
            snapshots_data[next_snapshot_key] = snapshot_record
            logger.info(f"âœ… ä¿å­˜ç¬¬ {snapshot_id} æ¬¡ refresh å¿«ç…§")
        
        # [Fix] ç¡®ä¿å­—æ®µé¡ºåºç¬¦åˆç”¨æˆ·è¦æ±‚
        ordered_data = {
            "symbol": snapshots_data.get("symbol", symbol),
            "start_date": snapshots_data.get("start_date", datetime.now().strftime("%Y-%m-%d")),
            "created_at": snapshots_data.get("created_at", datetime.now().isoformat()),
            "market_params": snapshots_data.get("market_params", {}),
            "dyn_params": snapshots_data.get("dyn_params", {}),
            "cluster_assessment": snapshots_data.get("cluster_assessment", {}),
            "source_target": snapshots_data.get("source_target"),
            "last_updated": datetime.now().isoformat()
        }
        
        # ä¿ç•™å…¶ä»–å·²æœ‰çš„ snapshots_N å­—æ®µ
        for key, value in snapshots_data.items():
            if key.startswith("snapshots_"):
                ordered_data[key] = value
        
        self._save_cache(cache_path, ordered_data)
        logger.success(f"ğŸ’¾ å¿«ç…§å·²ä¿å­˜: {cache_path}")
        
        return {
            "status": "success",
            "file_path": str(cache_path),
            "snapshot_file": str(cache_path),
            "snapshot": snapshot_record,
            "total_snapshots": sum(1 for k in ordered_data.keys() if k.startswith("snapshots_"))
        }

    def load_latest_greeks_snapshot(self, symbol: str) -> Optional[Dict]:
        """åŠ è½½æœ€æ–°çš„å¸Œè…Šå€¼å¿«ç…§"""
        safe_symbol = self._sanitize_symbol(symbol)
        snapshot_file = self._get_output_filename(safe_symbol)
        
        if not snapshot_file.exists():
            logger.warning(f"æœªæ‰¾åˆ°å¿«ç…§æ–‡ä»¶: {snapshot_file}")
            return None
        
        with open(snapshot_file, 'r', encoding='utf-8') as f:
            snapshots_data = json.load(f)
        
        # è·å–æœ€æ–°çš„å¿«ç…§
        snapshot_keys = [k for k in snapshots_data.keys() if k.startswith("snapshots_")]
        
        if not snapshot_keys:
            # å¦‚æœæ²¡æœ‰ refresh å¿«ç…§ï¼Œè¿”å› source_target
            return snapshots_data.get("source_target")
        
        # è¿”å›æœ€åä¸€ä¸ªå¿«ç…§
        latest_key = sorted(snapshot_keys, key=lambda x: int(x.split("_")[1]))[-1]
        return snapshots_data[latest_key]
    
    def get_all_snapshots(self, symbol: str) -> Optional[Dict]:
        """è·å–æ‰€æœ‰å¿«ç…§æ•°æ®"""
        # å¤ç”¨ load_analysisï¼Œå› ä¸ºå®ƒå·²ç»åŒ…å«äº†æŸ¥æ‰¾æœ€æ–°æ–‡ä»¶çš„é€»è¾‘
        return self.load_analysis(symbol)

    def add_backtest_record(self, symbol: str, record: Dict[str, Any], start_date: str = None):
        """æ·»åŠ å›æµ‹è®°å½•åˆ°ç¼“å­˜"""
        safe_symbol = self._sanitize_symbol(symbol)
        cached = self.load_analysis(safe_symbol, start_date)
        
        if not cached:
            logger.warning(f"æœªæ‰¾åˆ° {safe_symbol} çš„ç¼“å­˜ï¼Œæ— æ³•æ·»åŠ å›æµ‹è®°å½•")
            return
        
        if "backtest_records" not in cached:
            cached["backtest_records"] = []
        
        record["timestamp"] = datetime.now().isoformat()
        cached["backtest_records"].append(record)
        
        # é‡æ–°ä¿å­˜
        c_start_date = cached.get("start_date", "").replace("-", "")
        if not c_start_date:
             c_start_date = datetime.now().strftime("%Y%m%d")
             
        cache_path = self._get_output_filename(safe_symbol, c_start_date)
        self._save_cache(cache_path, cached)
        logger.info(f"âœ… å›æµ‹è®°å½•å·²æ·»åŠ ")

    # ============================================
    # æ·±åº¦å¯¹æ¯”é€»è¾‘ (Deep Comparison) - Phase 3 Enhanced
    # ============================================

    def compare_snapshots(self, symbol: str, from_num: int, to_num: int) -> Optional[Dict]:
        """
        [Enhanced] å¯¹æ¯”ä¸¤ä¸ªå¿«ç…§çš„å·®å¼‚ï¼ˆè¦†ç›– Phase 3 æ‰€æœ‰æ ¸å¿ƒç»´åº¦ï¼‰
        
        å¯¹æ¯”ç»´åº¦:
        1. åŸºç¡€: Spot Price, Vol Trigger
        2. ç»“æ„: Walls (Call/Put), Net GEX
        3. æµå‘ (New): DEX Bias, Vanna Dir, IV Path
        4. æ›²é¢ (New): Skew, Smile Steepness
        5. é£é™© (New): Volume Signal, Vega Exposure
        """
        safe_symbol = self._sanitize_symbol(symbol)
        snapshots_data = self.get_all_snapshots(safe_symbol)
        
        if not snapshots_data:
            logger.warning(f"æœªæ‰¾åˆ° {safe_symbol} çš„å¿«ç…§æ•°æ®")
            return None
        
        # è·å–èµ·å§‹å¿«ç…§
        if from_num == 0:
            from_snapshot = snapshots_data.get("source_target")
            from_label = "T0 (Baseline)"
        else:
            from_key = f"snapshots_{from_num}"
            from_snapshot = snapshots_data.get(from_key)
            from_label = f"T{from_num} (Snapshot)"
        
        # è·å–ç»“æŸå¿«ç…§
        to_key = f"snapshots_{to_num}"
        to_snapshot = snapshots_data.get(to_key)
        to_label = f"T{to_num} (Snapshot)"
        
        if not from_snapshot or not to_snapshot:
            logger.warning(f"å¿«ç…§ä¸å­˜åœ¨: {from_label} æˆ– {to_label}")
            return None
        
        from_targets = from_snapshot.get("targets", {})
        to_targets = to_snapshot.get("targets", {})
        
        changes = {}
        
        # 1. Spot Price (åŸºç¡€ä»·æ ¼)
        fp = from_targets.get("spot_price", 0)
        tp = to_targets.get("spot_price", 0)
        if fp != tp:
            pct = ((tp - fp) / fp) * 100 if fp else 0
            changes["spot_price"] = {
                "from": fp, 
                "to": tp, 
                "change": round(tp - fp, 2),
                "change_pct": round(pct, 2)
            }
            
        # 2. Gamma Metrics (GEX, Trigger)
        fg = from_targets.get("gamma_metrics", {})
        tg = to_targets.get("gamma_metrics", {})
        
        for k in ["net_gex", "vol_trigger", "gap_distance_dollar"]:
            fv, tv = fg.get(k), tg.get(k)
            if fv != tv:
                changes[f"gamma_metrics.{k}"] = {
                    "from": fv, 
                    "to": tv,
                    "change": round(tv - fv, 2) if isinstance(fv, (int, float)) else "N/A"
                }
        
        # spot_vs_trigger å˜åŒ– (String)
        if fg.get("spot_vs_trigger") != tg.get("spot_vs_trigger"):
            changes["gamma_metrics.spot_vs_trigger"] = {
                "from": fg.get("spot_vs_trigger"),
                "to": tg.get("spot_vs_trigger"),
                "changed": True
            }
        
        # 3. Walls (å…³é”®ç‚¹ä½)
        fw = from_targets.get("walls", {})
        tw = to_targets.get("walls", {})
        for k in ["call_wall", "put_wall", "major_wall"]:
            fv, tv = fw.get(k), tw.get(k)
            if fv != tv:
                changes[f"walls.{k}"] = {
                    "from": fv, 
                    "to": tv, 
                    "action": "SHIFT",
                    "change_pct": round((tv-fv)/fv*100, 1) if fv else 0
                }
                
        # 4. [Phase 3 New] Directional Metrics (Flow)
        fd = from_targets.get("directional_metrics", {})
        td = to_targets.get("directional_metrics", {})
        for k in ["dex_bias", "vanna_dir", "iv_path"]:
            fv, tv = fd.get(k), td.get(k)
            if fv != tv:
                changes[f"flow.{k}"] = {"from": fv, "to": tv, "alert": True}

        # 5. [Phase 3 New] Vol Surface (Skew/Smile)
        fv = from_targets.get("vol_surface", {})
        tv = to_targets.get("vol_surface", {})
        for k in ["smile_steepness", "skew_25d"]:
            fval, tval = fv.get(k), tv.get(k)
            if fval != tval:
                changes[f"vol.{k}"] = {"from": fval, "to": tval}

        # 6. [Phase 3 New] Validation Metrics (Risk)
        fval = from_targets.get("validation_metrics", {})
        tval = to_targets.get("validation_metrics", {})
        for k in ["net_volume_signal", "net_vega_exposure"]:
            f_item, t_item = fval.get(k), tval.get(k)
            if f_item != t_item:
                changes[f"risk.{k}"] = {"from": f_item, "to": t_item}
        
        # 7. ATM IV
        from_iv = from_targets.get("atm_iv", {})
        to_iv = to_targets.get("atm_iv", {})
        for k in ["iv_7d", "iv_14d"]:
            fv, tv = from_iv.get(k), to_iv.get(k)
            if fv != tv:
                changes[f"atm_iv.{k}"] = {"from": fv, "to": tv}

        return {
            "meta": {
                "symbol": symbol,
                "compare_pair": f"{from_label} vs {to_label}",
                "timestamp": datetime.now().isoformat()
            },
            "from_snapshot": {
                "id": from_num,
                "time": from_snapshot.get("timestamp"),
                "note": from_snapshot.get("note")
            },
            "to_snapshot": {
                "id": to_num,
                "time": to_snapshot.get("timestamp"),
                "note": to_snapshot.get("note")
            },
            "changes": changes,
            "change_count": len(changes)
        }

    @staticmethod
    def _get_nested_value(data: Dict, path: str):
        """è·å–åµŒå¥—å­—æ®µå€¼ï¼ˆæ”¯æŒç‚¹å·è·¯å¾„ï¼‰"""
        keys = path.split('.')
        value = data
        for key in keys:
            if isinstance(value, dict):
                value = value.get(key)
            else:
                return None
        return value if value != -999 else None