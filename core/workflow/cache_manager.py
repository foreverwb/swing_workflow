"""
ç¼“å­˜ç®¡ç†å™¨ï¼ˆä¿®å¤ç‰ˆï¼‰
èŒè´£ï¼š
1. ç®¡ç†å®Œæ•´åˆ†æç»“æœç¼“å­˜
2. ç®¡ç†å¸Œè…Šå€¼å¿«ç…§ï¼ˆæ”¯æŒå¤šæ¬¡ refreshï¼‰
3. å¿«ç…§å¯¹æ¯”åŠŸèƒ½

ä¿®å¤ï¼š
- initialize_cache_with_params: ç§»é™¤é”™è¯¯çš„ snapshots_1 é¢„åˆå§‹åŒ–
"""

import json
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime
from loguru import logger
import re


class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨"""
        # å®Œæ•´åˆ†æè¾“å‡ºç›®å½•
        self.output_dir = Path("data/output")
        # å…³é”®æ”¹åŠ¨ï¼šä»…åœ¨ä¸å­˜åœ¨æ—¶åˆ›å»º
        if not self.output_dir.exists():
            self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¸´æ—¶ç¼“å­˜ç›®å½•
        self.temp_dir = Path("data/temp")
        # å…³é”®æ”¹åŠ¨ï¼šä»…åœ¨ä¸å­˜åœ¨æ—¶åˆ›å»º
        if not self.temp_dir.exists():
            self.temp_dir.mkdir(parents=True, exist_ok=True)
    
    # ============================================
    # å®Œæ•´åˆ†æç»“æœç¼“å­˜
    # ============================================
    
    def _get_output_filename(self, symbol: str, start_date: str = None) -> Path:
        """
        è·å–è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆç»Ÿä¸€æ ¼å¼ï¼‰
        
        æ ¼å¼ï¼šdata/output/{SYMBOL}/{YYYYMMDD}/{SYMBOL}_{YYYYMMDD}.json
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            start_date: åˆ†æå¼€å§‹æ—¥æœŸï¼ˆYYYYMMDDï¼‰
            
        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if not start_date:
            start_date = datetime.now().strftime("%Y%m%d")
        
        # åˆ›å»ºæ—¥æœŸå­ç›®å½•
        symbol_dir = self.output_dir / symbol
        date_dir = symbol_dir / start_date
        if not date_dir.exists():
            logger.debug(f"ğŸ“ åˆ›å»ºç¼“å­˜ç›®å½•: {date_dir}")
            date_dir.mkdir(parents=True, exist_ok=True)
        
        return date_dir / f"{symbol}_{start_date}.json"
    
    def get_cache_file(self, symbol: str, start_date: str = None) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„ï¼ˆå‘åå…¼å®¹ï¼‰"""
        return self._get_output_filename(symbol, start_date)
    
    def load_analysis(self, symbol: str, start_date: str = None) -> Optional[Dict[str, Any]]:
        """
        åŠ è½½å®Œæ•´åˆ†æç»“æœ
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            start_date: åˆ†æå¼€å§‹æ—¥æœŸï¼ˆYYYYMMDDï¼‰ï¼Œä¸æŒ‡å®šåˆ™æŸ¥æ‰¾æœ€æ–°
            
        Returns:
            ç¼“å­˜æ•°æ®æˆ– None
        """
        if start_date:
            # åŠ è½½æŒ‡å®šæ—¥æœŸçš„åˆ†æ
            cache_file = self._get_output_filename(symbol, start_date)
        else:
            # æŸ¥æ‰¾æœ€æ–°çš„åˆ†ææ–‡ä»¶
            symbol_dir = self.output_dir / symbol
            if not symbol_dir.exists():
                return None
            
            analysis_files = sorted(symbol_dir.glob(f"{symbol}_*.json"), reverse=True)
            if not analysis_files:
                return None
            
            cache_file = analysis_files[0]
        
        if not cache_file.exists():
            return None
        
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return None
    
    def save_complete_analysis(
        self,
        symbol: str,
        initial_data: Dict,
        scenario: Dict,
        strategies: Dict,
        ranking: Dict,
        report: str,
        start_date: str = None,
        cache_file: str = None,
        market_params: Dict = None, 
        dyn_params: Dict = None,     
    ):
        """
        ä¿å­˜å®Œæ•´åˆ†æç»“æœåˆ° source_target
        """
        # éªŒè¯ symbol
        if not symbol or symbol.upper() == "UNKNOWN":
            logger.error(f"æ— æ•ˆçš„ symbol: '{symbol}'ï¼Œè·³è¿‡ä¿å­˜")
            return
        
        symbol = symbol.upper()
        
        if not start_date:
            start_date = datetime.now().strftime("%Y%m%d")
        
        # ç¡®å®šç¼“å­˜è·¯å¾„
        if cache_file:
            # ä»æ–‡ä»¶åæå– start_date
            match = re.match(r'(\w+)_(\d{8})\.json', cache_file)
            if match:
                start_date = match.group(2)
            
            symbol_dir = self.output_dir / symbol
            date_dir = symbol_dir / start_date
            
            # å…³é”®æ”¹åŠ¨ï¼šä»…åœ¨ä¸å­˜åœ¨æ—¶åˆ›å»º
            if not date_dir.exists():
                logger.debug(f"ğŸ“ åˆ›å»ºç¼“å­˜ç›®å½•: {date_dir}")
                date_dir.mkdir(parents=True, exist_ok=True)
            
            cache_path = date_dir / cache_file
        else:
            cache_path = self._get_output_filename(symbol, start_date)
        
        # åŠ è½½ç°æœ‰ç¼“å­˜
        if cache_path.exists():
            with open(cache_path, 'r', encoding='utf-8') as f:
                cached = json.load(f)
        else:
            # åˆ›å»ºæ–°ç¼“å­˜
            cached = {
                "symbol": symbol,
                "start_date": datetime.strptime(start_date, "%Y%m%d").strftime("%Y-%m-%d"),
                "created_at": datetime.now().isoformat()
            }
        if market_params and dyn_params:
            cached["market_params"] = {
                "vix": market_params.get("vix"),
                "ivr": market_params.get("ivr"),
                "iv30": market_params.get("iv30"),
                "hv20": market_params.get("hv20"),
                "vrp": market_params.get("iv30", 0) / market_params.get("hv20", 1) if market_params.get("hv20", 0) > 0 else 0,
                "updated_at": datetime.now().isoformat()
            }
            
            cached["dyn_params"] = {
                "dyn_strikes": dyn_params.get("dyn_strikes"),
                "dyn_dte_short": dyn_params.get("dyn_dte_short"),
                "dyn_dte_mid": dyn_params.get("dyn_dte_mid"),
                "dyn_dte_long_backup": dyn_params.get("dyn_dte_long_backup"),
                "dyn_window": dyn_params.get("dyn_window"),
                # "scenario": dyn_params.get("scenario"),
                "updated_at": datetime.now().isoformat()
            }
            
            logger.info(f"âœ… å¸‚åœºå‚æ•°å·²å†™å…¥ç¼“å­˜ | åœºæ™¯: {dyn_params.get('scenario')}")
        
        # å†™å…¥ source_target
        cached["source_target"] = {
            "timestamp": datetime.now().isoformat(),
            "data": initial_data,
            "scenario": scenario,
            "strategies": strategies,
            "ranking": ranking,
            "report": report
        }
        
        cached["last_updated"] = datetime.now().isoformat()
        
        # ä¿å­˜ç¼“å­˜
        try:
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(cached, f, ensure_ascii=False, indent=2)
        
            logger.success(f"âœ… å®Œæ•´åˆ†æç»“æœå·²ä¿å­˜: {cache_path}")
            logger.info(f"  â€¢ æ–‡ä»¶å¤§å°: {cache_path.stat().st_size / 1024:.2f} KB")
            logger.info(f"  â€¢ source_target: å·²å¡«å……")
            logger.info(f"  â€¢ market_params: {'å·²ä¿å­˜' if market_params else 'ç»§æ‰¿ç¼“å­˜'}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å¤±è´¥: {e}")
            raise
    
    def add_backtest_record(self, symbol: str, record: Dict[str, Any], start_date: str = None):
        """
        æ·»åŠ å›æµ‹è®°å½•
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            record: å›æµ‹è®°å½•
            start_date: åˆ†æå¼€å§‹æ—¥æœŸ
        """
        cached = self.load_analysis(symbol, start_date)
        
        if not cached:
            logger.warning(f"æœªæ‰¾åˆ° {symbol} çš„ç¼“å­˜ï¼Œæ— æ³•æ·»åŠ å›æµ‹è®°å½•")
            return
        
        if "backtest_records" not in cached:
            cached["backtest_records"] = []
        
        record["timestamp"] = datetime.now().isoformat()
        cached["backtest_records"].append(record)
        
        cache_file = self._get_output_filename(symbol, cached.get("start_date"))
        self._save_cache(cache_file, cached)
        logger.info(f"âœ… å›æµ‹è®°å½•å·²æ·»åŠ ")
    
    # ============================================
    # å¸Œè…Šå€¼å¿«ç…§ç®¡ç†ï¼ˆrefresh å¿«ç…§ï¼‰
    # ============================================
    
    def save_greeks_snapshot(
        self,
        symbol: str,
        data: Dict,
        note: str = "",
        is_initial: bool = False,
        cache_file_name: str = None
    ) -> Dict:
        """
        ä¿å­˜å¸Œè…Šå€¼å¿«ç…§ï¼ˆæ”¯æŒå¤šæ¬¡ refreshï¼‰
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            data: å®Œæ•´æ•°æ®
            note: å¤‡æ³¨
            is_initial: æ˜¯å¦ä¸ºåˆå§‹åˆ†æï¼ˆsource_targetï¼‰
            cache_file_name: ç¼“å­˜æ–‡ä»¶åï¼ˆå¦‚ NVDA_20251127.jsonï¼‰
            
        Returns:
            ä¿å­˜ç»“æœ
        """
        # éªŒè¯ symbol
        if not symbol or symbol.upper() == "UNKNOWN":
            logger.error(f"æ— æ•ˆçš„ symbol: '{symbol}'ï¼Œè·³è¿‡ä¿å­˜å¿«ç…§")
            return {
                "status": "error",
                "message": f"æ— æ•ˆçš„ symbol: {symbol}"
            }
        
        symbol = symbol.upper()
        
        # ç¡®å®šå¿«ç…§æ–‡ä»¶è·¯å¾„
        if cache_file_name:
            date_str = cache_file_name.replace(f"{symbol}_", "").replace(".json", "")
            snapshot_file = self._get_output_filename(symbol, date_str)
        else:
            snapshot_file = self._get_output_filename(symbol)
        
        # æå– targets æ•°æ®
        targets = data.get("targets", {})
        
        # è¯»å–ç°æœ‰å¿«ç…§æ–‡ä»¶
        if snapshot_file.exists():
            with open(snapshot_file, 'r', encoding='utf-8') as f:
                snapshots_data = json.load(f)
        else:
            # é¦–æ¬¡åˆ›å»º
            snapshots_data = {
                "symbol": symbol,
                "start_date": datetime.now().strftime("%Y-%m-%d"),
                "source_target": None
            }
        
        # è®¡ç®— snapshot_id
        if is_initial:
            snapshot_id = 0  # source_target çš„ ID ä¸º 0
        else:
            # ç»Ÿè®¡å·²æœ‰çš„ snapshots_N æ•°é‡
            snapshot_count = sum(1 for key in snapshots_data.keys() if key.startswith("snapshots_"))
            snapshot_id = snapshot_count + 1
        
        # åˆ›å»ºå¿«ç…§è®°å½•ï¼ˆæ·»åŠ  snapshot_idï¼‰
        snapshot_record = {
            "snapshot_id": snapshot_id,
            "timestamp": datetime.now().isoformat(),
            "note": note,
            "targets": targets
        }
        
        if is_initial:
            # ä¿å­˜åˆå§‹æ•°æ®åˆ° source_target
            snapshots_data["source_target"] = snapshot_record
            logger.info(f"âœ… ä¿å­˜åˆå§‹åˆ†ææ•°æ®åˆ° source_target")
        else:
            # ä¿å­˜åˆ° snapshots_N
            next_snapshot_key = f"snapshots_{snapshot_id}"
            snapshots_data[next_snapshot_key] = snapshot_record
            logger.info(f"âœ… ä¿å­˜ç¬¬ {snapshot_id} æ¬¡ refresh å¿«ç…§")
        
        # ä¿å­˜æ–‡ä»¶
        with open(snapshot_file, 'w', encoding='utf-8') as f:
            json.dump(snapshots_data, f, ensure_ascii=False, indent=2)
        
        logger.success(f"ğŸ’¾ å¿«ç…§å·²ä¿å­˜: {snapshot_file}")
        
        return {
            "status": "success",
            "snapshot_file": str(snapshot_file),
            "snapshot": snapshot_record,
            "total_snapshots": sum(1 for k in snapshots_data.keys() if k.startswith("snapshots_"))
        }
    
    def load_latest_greeks_snapshot(self, symbol: str) -> Optional[Dict]:
        """
        åŠ è½½æœ€æ–°çš„å¸Œè…Šå€¼å¿«ç…§
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            
        Returns:
            æœ€æ–°å¿«ç…§æ•°æ®ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å› None
        """
        snapshot_file = self._get_output_filename(symbol)
        
        if not snapshot_file.exists():
            logger.warning(f"æœªæ‰¾åˆ°å¿«ç…§æ–‡ä»¶: {snapshot_file}")
            return None
        
        with open(snapshot_file, 'r', encoding='utf-8') as f:
            snapshots_data = json.load(f)
        
        # è·å–æœ€æ–°çš„å¿«ç…§
        snapshot_keys = [k for k in snapshots_data.keys() if k.startswith("snapshots_")]
        
        if not snapshot_keys:
            # å¦‚æœæ²¡æœ‰ refresh å¿«ç…§ï¼Œè¿”å› source_target
            return snapshots_data.get("source_target")
        
        # è¿”å›æœ€åä¸€ä¸ªå¿«ç…§
        latest_key = sorted(snapshot_keys, key=lambda x: int(x.split("_")[1]))[-1]
        return snapshots_data[latest_key]
    
    def get_all_snapshots(self, symbol: str) -> Optional[Dict]:
        """
        è·å–æ‰€æœ‰å¿«ç…§æ•°æ®
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            
        Returns:
            å®Œæ•´çš„å¿«ç…§æ–‡ä»¶å†…å®¹
        """
        snapshot_file = self._get_output_filename(symbol)
        
        if not snapshot_file.exists():
            return None
        
        with open(snapshot_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    # ============================================
    # å¿«ç…§å¯¹æ¯”åŠŸèƒ½
    # ============================================
    
    def compare_snapshots(self, symbol: str, from_num: int, to_num: int) -> Optional[Dict]:
        """
        å¯¹æ¯”ä¸¤ä¸ªå¿«ç…§çš„å·®å¼‚
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            from_num: èµ·å§‹å¿«ç…§ç¼–å·ï¼ˆ0 è¡¨ç¤º source_targetï¼‰
            to_num: ç»“æŸå¿«ç…§ç¼–å·
            
        Returns:
            å¯¹æ¯”ç»“æœå­—å…¸
        """
        snapshots_data = self.get_all_snapshots(symbol)
        
        if not snapshots_data:
            logger.warning(f"æœªæ‰¾åˆ° {symbol} çš„å¿«ç…§æ•°æ®")
            return None
        
        # è·å–èµ·å§‹å¿«ç…§
        if from_num == 0:
            from_snapshot = snapshots_data.get("source_target")
            from_label = "source_target"
        else:
            from_key = f"snapshots_{from_num}"
            from_snapshot = snapshots_data.get(from_key)
            from_label = f"å¿«ç…§ #{from_num}"
        
        # è·å–ç»“æŸå¿«ç…§
        to_key = f"snapshots_{to_num}"
        to_snapshot = snapshots_data.get(to_key)
        to_label = f"å¿«ç…§ #{to_num}"
        
        if not from_snapshot or not to_snapshot:
            logger.warning(f"å¿«ç…§ä¸å­˜åœ¨: {from_label} æˆ– {to_label}")
            return None
        
        # æå– targets æ•°æ®
        from_targets = from_snapshot.get("targets", {})
        to_targets = to_snapshot.get("targets", {})
        
        # å¯¹æ¯”å…³é”®å­—æ®µ
        changes = {}
        
        # 1. spot_price
        from_price = from_targets.get("spot_price")
        to_price = to_targets.get("spot_price")
        if from_price and to_price and from_price != to_price:
            change_pct = ((to_price - from_price) / from_price) * 100
            changes["spot_price"] = {
                "from": from_price,
                "to": to_price,
                "change": round(to_price - from_price, 2),
                "change_pct": round(change_pct, 2)
            }
        
        # 2. gamma_metrics
        from_gamma = from_targets.get("gamma_metrics", {})
        to_gamma = to_targets.get("gamma_metrics", {})
        
        for field in ["net_gex", "vol_trigger", "gap_distance_dollar"]:
            from_val = from_gamma.get(field)
            to_val = to_gamma.get(field)
            if from_val and to_val and from_val != to_val:
                change_pct = ((to_val - from_val) / from_val) * 100 if from_val != 0 else 0
                changes[f"gamma_metrics.{field}"] = {
                    "from": from_val,
                    "to": to_val,
                    "change": round(to_val - from_val, 2),
                    "change_pct": round(change_pct, 2)
                }
        
        # spot_vs_trigger å˜åŒ–ï¼ˆå­—ç¬¦ä¸²ï¼‰
        from_trigger = from_gamma.get("spot_vs_trigger")
        to_trigger = to_gamma.get("spot_vs_trigger")
        if from_trigger != to_trigger:
            changes["gamma_metrics.spot_vs_trigger"] = {
                "from": from_trigger,
                "to": to_trigger,
                "changed": True
            }
        
        # 3. walls
        from_walls = from_targets.get("walls", {})
        to_walls = to_targets.get("walls", {})
        
        for field in ["call_wall", "put_wall", "major_wall"]:
            from_val = from_walls.get(field)
            to_val = to_walls.get(field)
            if from_val and to_val and from_val != to_val:
                change_pct = ((to_val - from_val) / from_val) * 100 if from_val != 0 else 0
                changes[f"walls.{field}"] = {
                    "from": from_val,
                    "to": to_val,
                    "change": round(to_val - from_val, 2),
                    "change_pct": round(change_pct, 2)
                }
        
        # 4. atm_iv
        from_iv = from_targets.get("atm_iv", {})
        to_iv = to_targets.get("atm_iv", {})
        
        for field in ["iv_7d", "iv_14d"]:
            from_val = from_iv.get(field)
            to_val = to_iv.get(field)
            if from_val and to_val and from_val != to_val:
                change_pct = ((to_val - from_val) / from_val) * 100 if from_val != 0 else 0
                changes[f"atm_iv.{field}"] = {
                    "from": from_val,
                    "to": to_val,
                    "change": round(to_val - from_val, 2),
                    "change_pct": round(change_pct, 2)
                }
        
        return {
            "from_snapshot": {
                "label": from_label,
                "timestamp": from_snapshot.get("timestamp"),
                "note": from_snapshot.get("note")
            },
            "to_snapshot": {
                "label": to_label,
                "timestamp": to_snapshot.get("timestamp"),
                "note": to_snapshot.get("note")
            },
            "changes": changes,
            "total_changes": len(changes)
        }
    
    # ============================================
    # è¾…åŠ©æ–¹æ³•
    # ============================================
    
    def _save_cache(self, cache_file: Path, data: Dict[str, Any]):
        """ä¿å­˜ç¼“å­˜åˆ°æ–‡ä»¶"""
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    @staticmethod
    def _get_nested_value(data: Dict, path: str):
        """è·å–åµŒå¥—å­—æ®µå€¼ï¼ˆæ”¯æŒç‚¹å·è·¯å¾„ï¼‰"""
        keys = path.split('.')
        value = data
        for key in keys:
            if isinstance(value, dict):
                value = value.get(key)
            else:
                return None
        return value if value != -999 else None
    
    def save_market_params(
        self,
        symbol: str,
        market_params: Dict[str, float],
        dyn_params: Dict[str, Any],
        start_date: str = None,
        cache_file: str = None
    ) -> Path:
        """
        ä¿å­˜å¸‚åœºå‚æ•°å’ŒåŠ¨æ€å‚æ•°åˆ°ç¼“å­˜æ–‡ä»¶
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            market_params: å¸‚åœºå‚æ•° (vix, ivr, iv30, hv20)
            dyn_params: åŠ¨æ€å‚æ•° (dyn_strikes, dyn_dte_short, ...)
            start_date: åˆ†æå¼€å§‹æ—¥æœŸï¼ˆYYYYMMDDï¼‰
            cache_file: æŒ‡å®šç¼“å­˜æ–‡ä»¶åï¼ˆå¦‚ NVDA_20251127.jsonï¼‰
        
        Returns:
            ç¼“å­˜æ–‡ä»¶è·¯å¾„
        """
        # éªŒè¯ symbol
        if not symbol or symbol.upper() == "UNKNOWN":
            logger.error(f"æ— æ•ˆçš„ symbol: '{symbol}'ï¼Œè·³è¿‡ä¿å­˜å¸‚åœºå‚æ•°")
            return None
        
        symbol = symbol.upper()
        
        if not start_date:
            start_date = datetime.now().strftime("%Y%m%d")
        
        # ç¡®å®šç¼“å­˜è·¯å¾„
        if cache_file:
            match = re.match(r'(\w+)_(\d{8})\.json', cache_file)
            if match:
                start_date = match.group(2)
            
            symbol_dir = self.output_dir / symbol
            date_dir = symbol_dir / start_date
            
            if not date_dir.exists():
                logger.debug(f"ğŸ“ åˆ›å»ºç¼“å­˜ç›®å½•: {date_dir}")
                date_dir.mkdir(parents=True, exist_ok=True)
            
            cache_path = date_dir / cache_file
        else:
            cache_path = self._get_output_filename(symbol, start_date)
        
        # åŠ è½½ç°æœ‰ç¼“å­˜
        if cache_path.exists():
            with open(cache_path, 'r', encoding='utf-8') as f:
                cached = json.load(f)
        else:
            # åˆ›å»ºæ–°ç¼“å­˜
            cached = {
                "symbol": symbol,
                "start_date": datetime.strptime(start_date, "%Y%m%d").strftime("%Y-%m-%d"),
                "created_at": datetime.now().isoformat()
            }
        
        cached["market_params"] = {
            "vix": market_params.get("vix"),
            "ivr": market_params.get("ivr"),
            "iv30": market_params.get("iv30"),
            "hv20": market_params.get("hv20"),
            "vrp": market_params.get("iv30", 0) / market_params.get("hv20", 1) if market_params.get("hv20", 0) > 0 else 0,
            "updated_at": datetime.now().isoformat()
        }
        
        cached["dyn_params"] = {
            "dyn_strikes": dyn_params.get("dyn_strikes"),
            "dyn_dte_short": dyn_params.get("dyn_dte_short"),
            "dyn_dte_mid": dyn_params.get("dyn_dte_mid"),
            "dyn_dte_long_backup": dyn_params.get("dyn_dte_long_backup"),
            "dyn_window": dyn_params.get("dyn_window"),
            "scenario": dyn_params.get("scenario"),
            "updated_at": datetime.now().isoformat()
        }
        
        cached["last_updated"] = datetime.now().isoformat()
        
        # ä¿å­˜ç¼“å­˜
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(cached, f, ensure_ascii=False, indent=2)
        
        logger.success(f"âœ… å¸‚åœºå‚æ•°å·²ä¿å­˜: {cache_path}")
        logger.info(f"   åœºæ™¯: {dyn_params.get('scenario')}")
        logger.info(f"   VRP: {cached['market_params']['vrp']:.2f}")
        
        return cache_path
    
    def load_market_params(self, symbol: str, start_date: str = None) -> Optional[Dict]:
        """
        åŠ è½½å¸‚åœºå‚æ•°
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            start_date: åˆ†æå¼€å§‹æ—¥æœŸï¼ˆYYYYMMDDï¼‰ï¼Œä¸æŒ‡å®šåˆ™æŸ¥æ‰¾æœ€æ–°
        
        Returns:
            åŒ…å« market_params å’Œ dyn_params çš„å­—å…¸ï¼Œä¸å­˜åœ¨è¿”å› None
        """
        cache_path = self._get_output_filename(symbol, start_date)
        
        if not cache_path.exists():
            logger.warning(f"ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_path}")
            return None
        
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                cached = json.load(f)
            
            if "market_params" not in cached or "dyn_params" not in cached:
                logger.warning(f"ç¼“å­˜æ–‡ä»¶ç¼ºå°‘å¸‚åœºå‚æ•°å­—æ®µ")
                return None
            
            return {
                "market_params": cached["market_params"],
                "dyn_params": cached["dyn_params"]
            }
        
        except Exception as e:
            logger.error(f"åŠ è½½å¸‚åœºå‚æ•°å¤±è´¥: {e}")
            return None
        
    def update_market_params_if_changed(
        self, 
        new_market_params: Dict[str, Any], 
        new_dyn_params: Dict[str, Any]
    ) -> bool:
        """
        ä»…å½“å‚æ•°å‘ç”Ÿå˜åŒ–æ—¶æ›´æ–°ç¼“å­˜
        
        Args:
            new_market_params: æ–°çš„å¸‚åœºå‚æ•°
            new_dyn_params: æ–°çš„åŠ¨æ€å‚æ•°
            
        Returns:
            bool: æ˜¯å¦å‘ç”Ÿäº†æ›´æ–°
        """
        try:
            old_market, old_dyn = self.load_market_params()
            
            # å¦‚æœä¸å­˜åœ¨æ—§å‚æ•°æˆ–å‚æ•°å‘ç”Ÿå˜åŒ–ï¼Œåˆ™æ›´æ–°
            if old_market != new_market_params or old_dyn != new_dyn_params:
                self.save_market_params(new_market_params, new_dyn_params)
                logger.info("å¸‚åœºå‚æ•°å·²æ›´æ–°ï¼ˆæ£€æµ‹åˆ°å˜åŒ–ï¼‰")
                return True
            
            logger.debug("å¸‚åœºå‚æ•°æœªå˜åŒ–ï¼Œè·³è¿‡æ›´æ–°")
            return False
            
        except Exception as e:
            logger.error(f"æ›´æ–°å¸‚åœºå‚æ•°å¤±è´¥: {e}")
            return False
        

    def initialize_cache_with_params(
        self,
        symbol: str,
        market_params: Dict[str, float],
        dyn_params: Dict[str, Any],
        start_date: str = None,
        tag: str = None
    ) -> Path:
        """
        åˆå§‹åŒ–ç¼“å­˜æ–‡ä»¶ï¼ˆä»…å¸‚åœºå‚æ•° + ç©ºå ä½ç¬¦ï¼‰
        ç”¨äº Agent2 å‘½ä»¤æ¸…å•ç”Ÿæˆååˆ›å»ºç¼“å­˜éª¨æ¶
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            market_params: å¸‚åœºå‚æ•° (vix, ivr, iv30, hv20)
            dyn_params: åŠ¨æ€å‚æ•° (dyn_strikes, dyn_dte_mid, ...)
            start_date: åˆ†æå¼€å§‹æ—¥æœŸï¼ˆYYYYMMDDï¼‰ï¼Œé»˜è®¤ä»Šå¤©
            tag: å·¥ä½œæµæ ‡è¯†ï¼ˆå¦‚ 'Meso'ï¼‰
            
        Returns:
            ç¼“å­˜æ–‡ä»¶è·¯å¾„
        """
        # éªŒè¯ symbol
        if not symbol or symbol.upper() == "UNKNOWN":
            logger.error(f"âŒ æ— æ•ˆçš„ symbol: '{symbol}'ï¼Œè·³è¿‡åˆå§‹åŒ–ç¼“å­˜")
            return None
        
        symbol = symbol.upper()
        
        if not start_date:
            start_date = datetime.now().strftime("%Y%m%d")
        
        # æ„é€ ç¼“å­˜è·¯å¾„
        cache_path = self._get_output_filename(symbol, start_date)
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if cache_path.exists():
            logger.warning(f"âš ï¸ ç¼“å­˜æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡åˆå§‹åŒ–: {cache_path}")
            return cache_path
        
        # åˆ›å»ºåˆå§‹ç¼“å­˜ç»“æ„
        cache_data = {
            "symbol": symbol,
            "start_date": datetime.strptime(start_date, "%Y%m%d").strftime("%Y-%m-%d"),
            "created_at": datetime.now().isoformat(),
            
            # âœ… å·¥ä½œæµæ ‡è¯†
            "tag": tag,
            
            # âœ… å¸‚åœºå‚æ•°
            "market_params": {
                "vix": market_params.get("vix"),
                "ivr": market_params.get("ivr"),
                "iv30": market_params.get("iv30"),
                "hv20": market_params.get("hv20"),
                "vrp": market_params.get("iv30", 0) / market_params.get("hv20", 1) if market_params.get("hv20", 0) > 0 else 0,
                "updated_at": datetime.now().isoformat()
            },
            
            # âœ… åŠ¨æ€å‚æ•°
            "dyn_params": {
                "dyn_strikes": dyn_params.get("dyn_strikes"),
                "dyn_dte_short": dyn_params.get("dyn_dte_short"),
                "dyn_dte_mid": dyn_params.get("dyn_dte_mid"),
                "dyn_dte_long_backup": dyn_params.get("dyn_dte_long_backup"),
                "dyn_window": dyn_params.get("dyn_window"),
                "scenario": dyn_params.get("scenario"),
                "updated_at": datetime.now().isoformat()
            },
            
            # âœ… ç©ºå ä½ç¬¦ï¼ˆç­‰å¾…åç»­å¡«å……ï¼‰
            "source_target": {},
            # ğŸ› ï¸ ä¿®å¤ï¼šç§»é™¤ snapshots_1 é¢„å ä½ï¼Œç”± save_greeks_snapshot åŠ¨æ€ç”Ÿæˆ
            # "snapshots_1": {},
            
            "last_updated": datetime.now().isoformat()
        }
        
        # ä¿å­˜æ–‡ä»¶
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            cache_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
            logger.success(f"âœ… åˆå§‹åŒ–ç¼“å­˜å·²åˆ›å»º: {cache_path}")
            if tag:
                logger.info(f"  â€¢ å·¥ä½œæµæ ‡è¯†: tag={tag}")
            logger.info(f"  â€¢ åœºæ™¯: {dyn_params.get('scenario')}")
            logger.info(f"  â€¢ VRP: {cache_data['market_params']['vrp']:.2f}")
            logger.info(f"  â€¢ æ–‡ä»¶å¤§å°: {cache_path.stat().st_size / 1024:.2f} KB")
            
            return cache_path
        
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–ç¼“å­˜å¤±è´¥: {e}")
            return None
        
    def load_market_params_from_cache(self, symbol: str, cache_file: str) -> Optional[Dict]:
        """
        ä»æŒ‡å®šç¼“å­˜æ–‡ä»¶åŠ è½½å¸‚åœºå‚æ•°
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            cache_file: ç¼“å­˜æ–‡ä»¶åï¼ˆå¦‚ NVDA_20251130.jsonï¼‰
        
        Returns:
            åŒ…å« market_params å’Œ dyn_params çš„å­—å…¸ï¼Œä¸å­˜åœ¨è¿”å› None
        """
        # ä»æ–‡ä»¶åæå–æ—¥æœŸ
        match = re.match(r'(\w+)_(\d{8})\.json', cache_file)
        if not match:
            logger.error(f"æ— æ•ˆçš„ç¼“å­˜æ–‡ä»¶åæ ¼å¼: {cache_file}ï¼Œåº”ä¸º SYMBOL_YYYYMMDD.json")
            return None
        
        start_date = match.group(2)
        
        # æ„é€ å®Œæ•´è·¯å¾„
        cache_path = self.output_dir / symbol / start_date / cache_file
        
        if not cache_path.exists():
            logger.warning(f"ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_path}")
            return None
        
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                cached = json.load(f)
            
            if "market_params" not in cached:
                logger.warning(f"ç¼“å­˜æ–‡ä»¶ç¼ºå°‘ market_params å­—æ®µ")
                return None
            
            if "dyn_params" not in cached:
                logger.warning(f"ç¼“å­˜æ–‡ä»¶ç¼ºå°‘ dyn_params å­—æ®µ")
                return None
            
            logger.info(f"âœ… ä»ç¼“å­˜åŠ è½½å¸‚åœºå‚æ•°: {cache_path}")
            
            return {
                "market_params": cached["market_params"],
                "dyn_params": cached["dyn_params"]
            }
        
        except Exception as e:
            logger.error(f"åŠ è½½å¸‚åœºå‚æ•°å¤±è´¥: {e}")
            return None