"""
CODE_AGGREGATOR - æ•°æ®èšåˆèŠ‚ç‚¹
æ”¯æŒå¤šæ¬¡ä¸Šä¼ æ•°æ®çš„å¢é‡åˆå¹¶

ä» YAML node id='1009' è¿ç§»
"""

import json
from datetime import datetime
from typing import Dict, List, Tuple, Any


def main(
    agent3_output: dict,
    first_parse_data: str = "",
    current_symbol: str = "",
    data_status: str = "initial",
    missing_count: int = 0,
    **env_vars
) -> dict:
    """
    æ•°æ®èšåˆèŠ‚ç‚¹ v5 - å¢é‡åˆå¹¶ä¼˜åŒ–ç‰ˆ
    
    æ ¸å¿ƒæ”¹è¿›:
    1. æ™ºèƒ½å¢é‡åˆå¹¶:å¤šæ¬¡ä¸Šä¼ è‡ªåŠ¨ç´¯ç§¯æ•°æ®
    2. å­—æ®µçº§è¿½è¸ª:è®°å½•æ¯ä¸ªå­—æ®µçš„æ¥æºå’Œè´¨é‡
    3. é˜²æ­¢è¦†ç›–:æœ‰æ•ˆæ•°æ®ä¸ä¼šè¢«æ— æ•ˆæ•°æ®è¦†ç›–
    4. è‡ªåŠ¨å®Œæˆ:è¾¾åˆ° 22/22 è‡ªåŠ¨è¿›å…¥åˆ†ææµç¨‹
    
    Args:
        agent3_output: Agent 3 çš„æ•°æ®æ ¡éªŒç»“æœ
        first_parse_data: é¦–æ¬¡è§£æçš„å®Œæ•´æ•°æ®(ç”¨äºç´¯ç§¯)
        current_symbol: å½“å‰åˆ†æçš„è‚¡ç¥¨ä»£ç 
        data_status: æ•°æ®çŠ¶æ€(initial | awaiting_data | ready)
        missing_count: ç¼ºå¤±å­—æ®µæ•°é‡
        **env_vars: ç¯å¢ƒå˜é‡
        
    Returns:
        {
            "result": èšåˆåçš„å®Œæ•´æ•°æ® JSON,
            "first_parse_data": æ›´æ–°åçš„ç¼“å­˜æ•°æ®,
            "current_symbol": è‚¡ç¥¨ä»£ç ,
            "data_status": æ–°çŠ¶æ€,
            "missing_count": ç¼ºå¤±æ•°é‡,
            "user_guide_summary": ç”¨æˆ·æŒ‡å¼•æ‘˜è¦,
            "user_guide_commands": éœ€è¦æ‰§è¡Œçš„å‘½ä»¤,
            "user_guide_progress": è¿›åº¦ä¿¡æ¯,
            ...
        }
    """
    try:
        # â­ æ–°å¢ï¼šè§„èŒƒåŒ–æ•°æ®ç»“æ„ï¼ˆå¤„ç† GPT-4o çš„å¹³é“ºç»“æ„ï¼‰
        current_data = _normalize_data_structure(agent3_output)
        
        # ğŸ” è°ƒè¯•æ—¥å¿— 1: æ£€æŸ¥è¾“å…¥æ•°æ®
        print(f"ğŸ“¥ è¾“å…¥æ•°æ®ç±»å‹: {type(current_data)}")
        print(f"ğŸ“¥ targets ç±»å‹: {type(current_data.get('targets'))}")
        
        # æ£€æŸ¥ targets æ˜¯å¦å·²ç»æ˜¯åµŒå¥—ç»“æ„
        targets = current_data.get('targets', {})
        if isinstance(targets, dict):
            has_nested = any(k in targets for k in ["gamma_metrics", "directional_metrics", "atm_iv", "walls"])
            print(f"ğŸ“¥ æ•°æ®ç»“æ„: {'åµŒå¥—ç»“æ„' if has_nested else 'å¹³é“ºç»“æ„ï¼ˆå·²è§„èŒƒåŒ–ï¼‰'}")
        
        symbol = extract_symbol(current_data)
        current_status = current_data.get("status", "missing_data")
        
        # === åˆ¤æ–­æ˜¯å¦ç´¯ç§¯æ¨¡å¼ ===
        is_accumulation_mode, judgment = judge_accumulation_mode(
            current_data=current_data,
            cached_first_data=first_parse_data,
            cached_symbol=current_symbol,
            cached_status=data_status
        )
        
        print(f"ğŸ“Š ç´¯ç§¯æ¨¡å¼: {is_accumulation_mode}, åŸå› : {judgment}")
        
        if is_accumulation_mode:
            # === å¢é‡åˆå¹¶ ===
            if not first_parse_data:
                # ç¬¬ä¸€æ¬¡ä¸Šä¼ 
                merged_data = current_data
                merge_history = [{
                    "round": 1,
                    "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "fields_added": count_valid_fields(current_data),
                    "action": "é¦–æ¬¡è§£æ"
                }]
                last_merge_failed = False
            else:
                # ç¬¬ N æ¬¡ä¸Šä¼  - æ‰§è¡Œå¢é‡åˆå¹¶
                first_data = json.loads(first_parse_data)
                
                # ğŸ” è°ƒè¯•æ—¥å¿— 2: åˆå¹¶å‰çš„æ•°æ®ç»Ÿè®¡
                first_count = count_valid_fields(first_data)
                new_count = count_valid_fields(current_data)
                print(f"ğŸ“Š åˆå¹¶å‰: ç¼“å­˜æ•°æ® {first_count} ä¸ªå­—æ®µ, æ–°æ•°æ® {new_count} ä¸ªå­—æ®µ")
                
                merged_data, merge_info = smart_merge(first_data, current_data)
                
                # ğŸ” è°ƒè¯•æ—¥å¿— 3: åˆå¹¶åçš„ç»“æœ
                merged_count = count_valid_fields(merged_data)
                print(f"ğŸ“Š åˆå¹¶å: {merged_count} ä¸ªå­—æ®µ")
                print(f"ğŸ“Š æ–°å¢: {merge_info['new_fields_count']}, æ›´æ–°: {merge_info['updated_fields_count']}")
                
                # æ›´æ–°åˆå¹¶å†å²
                history = first_data.get("_merge_history", [])
                history.append({
                    "round": len(history) + 1,
                    "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "fields_added": merge_info["new_fields_count"],
                    "fields_updated": merge_info.get("updated_fields_count", 0),
                    "action": "å¢é‡è¡¥é½" if not merge_info.get("merge_failed") else "åˆå¹¶å¤±è´¥",
                    "failure_reason": merge_info.get("failure_reason", "")
                })
                merged_data["_merge_history"] = history
                merge_history = history
                last_merge_failed = merge_info.get("merge_failed", False)
        else:
            # æ–°ä»»åŠ¡
            merged_data = current_data
            merge_history = [{
                "round": 1,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "fields_added": count_valid_fields(current_data),
                "action": "æ–°ä»»åŠ¡å¼€å§‹"
            }]
            last_merge_failed = False
        
        # === éªŒè¯ ===
        validation_result = enhanced_validation_v2(merged_data)
        
        # ğŸ” è°ƒè¯•æ—¥å¿— 4: éªŒè¯ç»“æœ
        print(f"âœ… éªŒè¯ç»“æœ: å®Œæˆç‡ {validation_result['summary']['completion_rate']}%")
        print(f"âœ… æä¾›å­—æ®µ: {validation_result['summary']['provided']}/{validation_result['summary']['total_required']}")
        
        if not isinstance(current_data, dict):
            raise ValueError(f"agent3_output ç±»å‹é”™è¯¯: {type(current_data)}")
        
        # æ›´æ–°çŠ¶æ€
        if validation_result["is_complete"]:
            final_status = "ready"
            merged_data["status"] = "data_ready"
        else:
            final_status = "awaiting_data"
            merged_data["status"] = "missing_data"
        
        # ç”Ÿæˆè¡¥é½æŒ‡å¼•(åŸºäºå®é™…ç¼ºå¤±å­—æ®µ)
        missing_fields = validation_result["missing_fields"]
        è¡¥é½æŒ‡å¼• = generate_smart_guide(
            missing_fields=missing_fields,
            merge_history=merge_history,
            total_fields=22,
            last_merge_failed=last_merge_failed,
            symbol=symbol
        )
        
        # === è¾“å‡ºç»“æ„åŒ–çš„ç»“æœ ===
        result_data = {
            **merged_data,
            "validation_summary": validation_result["summary"],
            "åˆ¤æ–­ä¾æ®": judgment,
            "_merge_history": merge_history
        }
        
        return {
            "result": json.dumps(result_data, ensure_ascii=False, indent=2),
            
            # ä¼šè¯å˜é‡
            "first_parse_data": json.dumps(merged_data, ensure_ascii=False) if final_status == "awaiting_data" else "",
            "current_symbol": symbol,
            "data_status": final_status,
            "missing_count": len(missing_fields),
            
            # è¡¥é½æŒ‡å¼•(æ‰å¹³åŒ–è¾“å‡º)
            "user_guide_summary": è¡¥é½æŒ‡å¼•.get("summary", ""),
            "user_guide_commands": è¡¥é½æŒ‡å¼•.get("commands_text", ""),
            "user_guide_progress": è¡¥é½æŒ‡å¼•.get("progress", ""),
            "user_guide_priority_critical": è¡¥é½æŒ‡å¼•.get("critical_text", ""),
            "user_guide_priority_high": è¡¥é½æŒ‡å¼•.get("high_text", ""),
            "user_guide_priority_medium": è¡¥é½æŒ‡å¼•.get("medium_text", ""),
            "user_guide_next_action": è¡¥é½æŒ‡å¼•.get("next_action", ""),
            "user_guide_merge_log": è¡¥é½æŒ‡å¼•.get("merge_log", "")
        }
    
    except Exception as e:
        import traceback
        return {
            "result": json.dumps({
                "error": True,
                "error_message": str(e),
                "error_traceback": traceback.format_exc()
            }, ensure_ascii=False, indent=2),
            "first_parse_data": first_parse_data,
            "current_symbol": current_symbol,
            "data_status": "error",
            "missing_count": 0,
            "user_guide_summary": f"âš ï¸ ç³»ç»Ÿé”™è¯¯: {str(e)}",
            "user_guide_commands": "",
            "user_guide_progress": "",
            "user_guide_priority_critical": "",
            "user_guide_priority_high": "",
            "user_guide_priority_medium": "",
            "user_guide_next_action": "è¯·æ£€æŸ¥æ•°æ®åé‡è¯•",
            "user_guide_merge_log": ""
        }


# ============= â­ æ–°å¢ï¼šæ•°æ®ç»“æ„è§„èŒƒåŒ– =============

def _normalize_data_structure(data: dict) -> dict:
    """
    å°†å¹³é“ºç»“æ„çš„æ•°æ®è½¬æ¢ä¸ºæ ‡å‡†åµŒå¥—ç»“æ„
    
    å¤„ç† GPT-4o ç­‰æ¨¡å‹è¿”å›çš„ä¸ç¬¦åˆ Schema çš„å¹³é“ºç»“æ„æ•°æ®
    
    Args:
        data: åŸå§‹æ•°æ®ï¼ˆå¯èƒ½æ˜¯å¹³é“ºæˆ–åµŒå¥—ç»“æ„ï¼‰
        
    Returns:
        æ ‡å‡†åµŒå¥—ç»“æ„çš„æ•°æ®
    """
    targets = data.get("targets", {})
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯åµŒå¥—ç»“æ„
    has_nested = any(k in targets for k in ["gamma_metrics", "directional_metrics", "atm_iv", "walls"])
    
    if has_nested:
        return data  # å·²ç»æ˜¯æ ‡å‡†ç»“æ„ï¼Œæ— éœ€è½¬æ¢
    
    # è½¬æ¢å¹³é“ºç»“æ„ä¸ºåµŒå¥—ç»“æ„
    normalized_targets = {
        "symbol": targets.get("symbol", "UNKNOWN"),
        "status": targets.get("status", "missing_data"),
        "spot_price": targets.get("spot_price", -999),
        "em1_dollar": targets.get("em1_dollar", -999),
        
        "walls": {
            "call_wall": targets.get("call_wall", -999),
            "put_wall": targets.get("put_wall", -999),
            "major_wall": targets.get("major_wall", -999),
            "major_wall_type": targets.get("major_wall_type", "N/A")
        },
        
        "gamma_metrics": {
            "gap_distance_dollar": targets.get("gap_distance_dollar", -999),
            "gap_distance_em1_multiple": targets.get("gap_distance_em1_multiple", -999),
            "cluster_strength_ratio": targets.get("cluster_strength_ratio", -999),
            "net_gex": targets.get("net_gex", -999),
            "net_gex_sign": targets.get("net_gex_sign", "N/A"),
            "vol_trigger": targets.get("vol_trigger", -999),
            "spot_vs_trigger": targets.get("spot_vs_trigger", "N/A"),
            "monthly_cluster_override": targets.get("monthly_cluster_override", "false")
        },
        
        "directional_metrics": {
            "dex_same_dir_pct": targets.get("dex_same_dir_pct", -999),
            "vanna_dir": targets.get("vanna_dir", "N/A"),
            "vanna_confidence": targets.get("vanna_confidence", "N/A"),
            "iv_path": targets.get("iv_path", "æ•°æ®ä¸è¶³"),
            "iv_path_confidence": targets.get("iv_path_confidence", "low")
        },
        
        "atm_iv": {
            "iv_7d": targets.get("iv_7d", -999),
            "iv_14d": targets.get("iv_14d", -999),
            "iv_source": targets.get("iv_source", "N/A")
        }
    }
    
    # ä¿ç•™å…¶ä»–å¯é€‰å­—æ®µ
    for key in ["validation_summary", "indices", "technical_analysis", "chart_metadata", "missing_fields", "è¡¥é½æŒ‡å¼•"]:
        if key in targets:
            normalized_targets[key] = targets[key]
    
    return {
        **data,
        "targets": normalized_targets
    }


# ============= æ ¸å¿ƒå‡½æ•° 1: æ™ºèƒ½åˆ¤æ–­ç´¯ç§¯æ¨¡å¼ =============

def judge_accumulation_mode(
    current_data: dict,
    cached_first_data: str,
    cached_symbol: str,
    cached_status: str
) -> Tuple[bool, str]:
    """
    åˆ¤æ–­æ˜¯å¦è¿›å…¥ç´¯ç§¯æ¨¡å¼(å¢é‡è¡¥é½)
    
    Returns:
        (is_accumulation, reason)
    """
    # æƒ…å†µ 1: æ— ç¼“å­˜ â†’ é¦–æ¬¡è§£æ
    if not cached_first_data or cached_status == "initial":
        return True, "é¦–æ¬¡ä¸Šä¼ ,å¼€å§‹è§£æ"
    
    # æƒ…å†µ 2: Symbol å˜åŒ– â†’ æ–°ä»»åŠ¡
    current_symbol = extract_symbol(current_data)
    if current_symbol != cached_symbol:
        return False, f"Symbol å˜åŒ–({cached_symbol}â†’{current_symbol}),å¼€å§‹æ–°ä»»åŠ¡"
    
    # æƒ…å†µ 3: ç¼“å­˜çŠ¶æ€ä¸º ready â†’ å·²å®Œæˆ,ä¸å†ç´¯ç§¯
    if cached_status == "ready":
        return False, "æ•°æ®å·²å®Œæ•´,å¼€å§‹æ–°ä»»åŠ¡"
    
    # æƒ…å†µ 4: ç¼“å­˜ç­‰å¾…è¡¥é½ â†’ ç´¯ç§¯æ¨¡å¼
    if cached_status == "awaiting_data":
        return True, "æ£€æµ‹åˆ°å†å²ç¼“å­˜,è¿›å…¥å¢é‡è¡¥é½æ¨¡å¼"
    
    # é»˜è®¤: ç´¯ç§¯æ¨¡å¼
    return True, "è¿›å…¥ç´¯ç§¯æ¨¡å¼"


# ============= æ ¸å¿ƒå‡½æ•° 2: æ™ºèƒ½åˆå¹¶ç®—æ³• =============

def smart_merge(first_data: dict, new_data: dict) -> Tuple[dict, dict]:
    """
    æ™ºèƒ½å¢é‡åˆå¹¶(å¢å¼ºç‰ˆ)
    
    æ–°å¢ç‰¹æ€§:
    1. æ£€æµ‹æ–°æ•°æ®æ˜¯å¦ä¸ºç©º
    2. å¦‚æœæ–°æ•°æ®ä¸ºç©º,ç›´æ¥è¿”å›æ—§æ•°æ®(ä¸åˆå¹¶)
    3. è®°å½•åˆå¹¶å¤±è´¥çš„åŸå› 
    """
    merged = first_data.copy()
    
    # æå– targets
    first_targets = get_target_dict(first_data)
    new_targets = get_target_dict(new_data)
    
    # ğŸ”¥ æ ¸å¿ƒä¿®å¤:æ£€æµ‹æ–°æ•°æ®æ˜¯å¦ä¸ºç©º
    new_valid_count = count_valid_fields_in_dict(new_targets)
    
    if new_valid_count == 0:
        # æ–°æ•°æ®ä¸ºç©º,ä¸æ‰§è¡Œåˆå¹¶
        print("âš ï¸ è­¦å‘Š: æ–°æ•°æ®æ— æœ‰æ•ˆå­—æ®µ,è·³è¿‡åˆå¹¶")
        merge_info = {
            "new_fields_count": 0,
            "updated_fields_count": 0,
            "merge_failed": True,
            "failure_reason": "æ–°æ•°æ®æ— æœ‰æ•ˆå­—æ®µ(å¯èƒ½è§£æå¤±è´¥)"
        }
        return merged, merge_info  # è¿”å›åŸæ•°æ®
    
    # ç»Ÿè®¡ä¿¡æ¯
    new_fields_count = 0
    updated_fields_count = 0
    
    # åˆå¹¶å„ä¸ª section
    for section in ["gamma_metrics", "directional_metrics", "atm_iv", "walls"]:
        if section not in first_targets:
            first_targets[section] = {}
        
        if section in new_targets:
            for key, new_value in new_targets[section].items():
                old_value = first_targets[section].get(key)
                
                if is_valid_value(new_value):
                    if not is_valid_value(old_value):
                        first_targets[section][key] = new_value
                        new_fields_count += 1
                    elif old_value != new_value:
                        first_targets[section][key] = new_value
                        updated_fields_count += 1
    
    # åˆå¹¶é¡¶å±‚å­—æ®µ
    for key in ["spot_price", "em1_dollar", "symbol"]:
        old_value = first_targets.get(key)
        new_value = new_targets.get(key)
        
        if is_valid_value(new_value):
            if not is_valid_value(old_value):
                first_targets[key] = new_value
                new_fields_count += 1
            elif old_value != new_value:
                first_targets[key] = new_value
                updated_fields_count += 1
    
    # ğŸ”¥ ä¿®å¤:å¦‚æœæ²¡æœ‰ä»»ä½•æ–°å¢æˆ–æ›´æ–°,æ ‡è®°ä¸ºå¤±è´¥
    if new_fields_count == 0 and updated_fields_count == 0:
        print("âš ï¸ è­¦å‘Š: åˆå¹¶æœªäº§ç”Ÿä»»ä½•å˜åŒ–,å¯èƒ½æ•°æ®é‡å¤æˆ–è§£æå¤±è´¥")
        merge_info = {
            "new_fields_count": 0,
            "updated_fields_count": 0,
            "merge_failed": True,
            "failure_reason": "æ— æ–°å¢æˆ–æ›´æ–°å­—æ®µ"
        }
        return merged, merge_info
    
    # åˆå¹¶ indices
    if "indices" not in merged:
        merged["indices"] = {}
    
    if "indices" in new_data:
        for index_name in ["spx", "qqq"]:
            if index_name in new_data["indices"]:
                if index_name not in merged["indices"]:
                    merged["indices"][index_name] = {}
                
                for key, new_value in new_data["indices"][index_name].items():
                    old_value = merged["indices"][index_name].get(key)
                    if is_valid_value(new_value) and not is_valid_value(old_value):
                        merged["indices"][index_name][key] = new_value
    
    # åˆå¹¶æŠ€æœ¯é¢æ•°æ®
    if "technical_analysis" in new_data:
        ta = new_data["technical_analysis"]
        if ta and ta.get("ta_score", 0) > 0:
            merged["technical_analysis"] = ta
    
    # æ›´æ–° targets
    merged["targets"] = first_targets
    
    merge_info = {
        "new_fields_count": new_fields_count,
        "updated_fields_count": updated_fields_count,
        "merge_failed": False
    }
    
    return merged, merge_info


# ============= è¾…åŠ©å‡½æ•° =============

def count_valid_fields_in_dict(target_dict: dict) -> int:
    """
    ç»Ÿè®¡å­—å…¸ä¸­çš„æœ‰æ•ˆå­—æ®µæ•°é‡ï¼ˆå¢å¼ºç‰ˆï¼‰
    
    æ”¯æŒä¸¤ç§æ•°æ®ç»“æ„ï¼š
    1. æ ‡å‡†åµŒå¥—ç»“æ„ï¼ˆSchema è§„å®šï¼‰
    2. å¹³é“ºç»“æ„ï¼ˆéƒ¨åˆ†æ¨¡å‹è¿”å›ï¼‰
    """
    count = 0
    
    # === å°è¯•æ ‡å‡†åµŒå¥—ç»“æ„ ===
    nested_count = 0
    for section in ["gamma_metrics", "directional_metrics", "atm_iv", "walls"]:
        if section in target_dict and isinstance(target_dict[section], dict):
            for value in target_dict[section].values():
                if is_valid_value(value):
                    nested_count += 1
    
    # æ£€æŸ¥é¡¶å±‚å¿…éœ€å­—æ®µ
    for key in ["spot_price", "em1_dollar"]:
        if is_valid_value(target_dict.get(key)):
            nested_count += 1
    
    # === å¦‚æœåµŒå¥—ç»“æ„å­˜åœ¨ï¼Œä½¿ç”¨åµŒå¥—è®¡æ•° ===
    if nested_count > 0:
        return nested_count
    
    # === å¦åˆ™å°è¯•å¹³é“ºç»“æ„ ===
    flat_required_fields = [
        "spot_price", "em1_dollar",
        # walls
        "call_wall", "put_wall", "major_wall", "major_wall_type",
        # gamma_metrics
        "gap_distance_dollar", "gap_distance_em1_multiple", 
        "cluster_strength_ratio", "net_gex", "net_gex_sign",
        "vol_trigger", "spot_vs_trigger", "monthly_cluster_override",
        # directional_metrics
        "dex_same_dir_pct", "vanna_dir", "vanna_confidence",
        "iv_path", "iv_path_confidence",
        # atm_iv
        "iv_7d", "iv_14d", "iv_source"
    ]
    
    flat_count = 0
    for field in flat_required_fields:
        if is_valid_value(target_dict.get(field)):
            flat_count += 1
    
    return flat_count


def is_valid_value(value: Any) -> bool:
    """åˆ¤æ–­å€¼æ˜¯å¦æœ‰æ•ˆ(éç¼ºå¤±)"""
    if value is None:
        return False
    if value == -999:
        return False
    if value in ["N/A", "æ•°æ®ä¸è¶³", "", "unknown"]:
        return False
    return True


def get_target_dict(data: dict) -> dict:
    """
    æå– targets å­—å…¸(å¢å¼ºé˜²å¾¡æ€§)
    
    è¿”å›ä¼˜å…ˆçº§:
    1. å¦‚æœ targets æ˜¯éç©ºå­—å…¸ â†’ ç›´æ¥è¿”å›
    2. å¦‚æœ targets æ˜¯éç©ºåˆ—è¡¨ â†’ è¿”å›ç¬¬ä¸€ä¸ªå…ƒç´ 
    3. å¦‚æœ targets ä¸ºç©ºæˆ–ç¼ºå¤± â†’ è¿”å›ç©ºå­—å…¸(ä½†ä¼šåœ¨æ—¥å¿—ä¸­è­¦å‘Š)
    """
    targets = data.get("targets")
    
    # ä¼˜å…ˆçº§1: ç›´æ¥æ˜¯å­—å…¸
    if isinstance(targets, dict) and targets:
        return targets
    
    # ä¼˜å…ˆçº§2: éç©ºåˆ—è¡¨
    if isinstance(targets, list) and targets:
        return targets[0] if isinstance(targets[0], dict) else {}
    
    # ä¼˜å…ˆçº§3: å›é€€åˆ°æ ¹èŠ‚ç‚¹ï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰
    # å¦‚æœdataæœ¬èº«åŒ…å«spot_priceç­‰å­—æ®µï¼Œè¯´æ˜targetså°±æ˜¯æ ¹èŠ‚ç‚¹
    if "spot_price" in data or "symbol" in data:
        print("âš ï¸ targetså­—æ®µç¼ºå¤±ï¼Œå°è¯•ä»æ ¹èŠ‚ç‚¹è¯»å–")
        return data
    
    # æ— æ³•è¯†åˆ«
    print(f"âŒ æ— æ³•æå–targetsï¼Œç±»å‹: {type(targets)}")
    return {}


def enhanced_validation_v2(data: dict) -> dict:
    """
    ä¸‰çº§éªŒè¯å¢å¼ºç‰ˆ(æ”¯æŒå¹³é“ºå’ŒåµŒå¥—ç»“æ„)
    
    Returns:
        {
            "is_complete": bool,
            "missing_fields": list,
            "summary": dict
        }
    """
    target = get_target_dict(data)
    
    # â­ æ£€æµ‹æ•°æ®ç»“æ„ç±»å‹
    is_nested = any(k in target for k in ["gamma_metrics", "directional_metrics", "atm_iv", "walls"])
    
    if is_nested:
        # === æ ‡å‡†åµŒå¥—ç»“æ„éªŒè¯ ===
        required_fields = {
            # é¡¶å±‚å­—æ®µ
            "spot_price": (target, "spot_price"),
            "em1_dollar": (target, "em1_dollar"),
            
            # walls
            "walls.call_wall": (target.get("walls", {}), "call_wall"),
            "walls.put_wall": (target.get("walls", {}), "put_wall"),
            "walls.major_wall": (target.get("walls", {}), "major_wall"),
            "walls.major_wall_type": (target.get("walls", {}), "major_wall_type"),
            
            # gamma_metrics
            "gamma_metrics.gap_distance_dollar": (target.get("gamma_metrics", {}), "gap_distance_dollar"),
            "gamma_metrics.gap_distance_em1_multiple": (target.get("gamma_metrics", {}), "gap_distance_em1_multiple"),
            "gamma_metrics.cluster_strength_ratio": (target.get("gamma_metrics", {}), "cluster_strength_ratio"),
            "gamma_metrics.net_gex": (target.get("gamma_metrics", {}), "net_gex"),
            "gamma_metrics.net_gex_sign": (target.get("gamma_metrics", {}), "net_gex_sign"),
            "gamma_metrics.vol_trigger": (target.get("gamma_metrics", {}), "vol_trigger"),
            "gamma_metrics.spot_vs_trigger": (target.get("gamma_metrics", {}), "spot_vs_trigger"),
            "gamma_metrics.monthly_cluster_override": (target.get("gamma_metrics", {}), "monthly_cluster_override"),
            
            # directional_metrics
            "directional_metrics.dex_same_dir_pct": (target.get("directional_metrics", {}), "dex_same_dir_pct"),
            "directional_metrics.vanna_dir": (target.get("directional_metrics", {}), "vanna_dir"),
            "directional_metrics.vanna_confidence": (target.get("directional_metrics", {}), "vanna_confidence"),
            "directional_metrics.iv_path": (target.get("directional_metrics", {}), "iv_path"),
            "directional_metrics.iv_path_confidence": (target.get("directional_metrics", {}), "iv_path_confidence"),
            
            # atm_iv
            "atm_iv.iv_7d": (target.get("atm_iv", {}), "iv_7d"),
            "atm_iv.iv_14d": (target.get("atm_iv", {}), "iv_14d"),
            "atm_iv.iv_source": (target.get("atm_iv", {}), "iv_source"),
        }
    else:
        # === â­ å¹³é“ºç»“æ„éªŒè¯ ===
        required_fields = {
            "spot_price": (target, "spot_price"),
            "em1_dollar": (target, "em1_dollar"),
            "call_wall": (target, "call_wall"),
            "put_wall": (target, "put_wall"),
            "major_wall": (target, "major_wall"),
            "major_wall_type": (target, "major_wall_type"),
            "gap_distance_dollar": (target, "gap_distance_dollar"),
            "gap_distance_em1_multiple": (target, "gap_distance_em1_multiple"),
            "cluster_strength_ratio": (target, "cluster_strength_ratio"),
            "net_gex": (target, "net_gex"),
            "net_gex_sign": (target, "net_gex_sign"),
            "vol_trigger": (target, "vol_trigger"),
            "spot_vs_trigger": (target, "spot_vs_trigger"),
            "monthly_cluster_override": (target, "monthly_cluster_override"),
            "dex_same_dir_pct": (target, "dex_same_dir_pct"),
            "vanna_dir": (target, "vanna_dir"),
            "vanna_confidence": (target, "vanna_confidence"),
            "iv_path": (target, "iv_path"),
            "iv_path_confidence": (target, "iv_path_confidence"),
            "iv_7d": (target, "iv_7d"),
            "iv_14d": (target, "iv_14d"),
            "iv_source": (target, "iv_source")
        }
    
    # æ£€æŸ¥ç¼ºå¤±å­—æ®µ
    missing_fields = []
    for field_path, (parent_dict, key) in required_fields.items():
        value = parent_dict.get(key) if isinstance(parent_dict, dict) else None
        if not is_valid_value(value):
            missing_fields.append({
                "field": field_path,
                "current_value": value
            })
    
    total_required = len(required_fields)
    provided = total_required - len(missing_fields)
    completion_rate = int((provided / total_required) * 100)
    
    is_complete = len(missing_fields) == 0
    
    return {
        "is_complete": is_complete,
        "missing_fields": missing_fields,
        "summary": {
            "total_required": total_required,
            "provided": provided,
            "missing_count": len(missing_fields),
            "completion_rate": completion_rate
        }
    }


def generate_smart_guide(
    missing_fields: list,
    merge_history: list,
    total_fields: int,
    last_merge_failed: bool = False,
    symbol: str = ''
) -> dict:
    """
    ç”Ÿæˆæ™ºèƒ½è¡¥é½æŒ‡å¼•
    
    æ–°å¢ç‰¹æ€§:
    1. æ˜¾ç¤ºç´¯ç§¯è¿›åº¦
    2. æ˜¾ç¤ºåˆå¹¶å†å²
    3. ä¼˜å…ˆçº§åŠ¨æ€è°ƒæ•´
    """
    if not missing_fields:
        return {
            "summary": "âœ… æ•°æ®å®Œæ•´,æ— éœ€è¡¥é½",
            "commands_text": "æ— ",
            "progress": f"100% ({total_fields}/{total_fields})",
            "critical_text": "æ— ",
            "high_text": "æ— ",
            "medium_text": "æ— ",
            "next_action": "è¿›å…¥åˆ†ææµç¨‹",
            "merge_log": format_merge_history(merge_history)
        }
    
    provided_count = total_fields - len(missing_fields)
    progress = f"{int((provided_count/total_fields)*100)}% ({provided_count}/{total_fields})"

    warning = ""
    if last_merge_failed:
        warning = "\n\nâš ï¸ **è­¦å‘Š**: ä¸Šæ¬¡ä¸Šä¼ çš„æ•°æ®æœªèƒ½æˆåŠŸè¯†åˆ«,è¯·ç¡®ä¿:\n" \
                  "1. å›¾ç‰‡æ¸…æ™°å®Œæ•´\n" \
                  "2. åŒ…å«ç›®æ ‡è‚¡ç¥¨çš„æœŸæƒæ•°æ®\n" \
                  "3. å‘½ä»¤æ‰§è¡Œç»“æœå®Œæ•´æ˜¾ç¤º"
    
    # æ ¹æ®å­—æ®µè·¯å¾„ç”Ÿæˆå‘½ä»¤å»ºè®®
    commands = []
    priority_groups = {"critical":[], "high": [], "medium": []}
    
    for item in missing_fields:
        field_path = item["field"]
        cmd_info = suggest_command(field_path, symbol)
        
        priority = cmd_info["priority"]
        priority_groups[priority].append({
            "å­—æ®µ": field_path,
            "å‘½ä»¤": cmd_info["command"],
            "è¯´æ˜": cmd_info["description"]
        })
        
        if cmd_info["command"] not in commands:
            commands.append(cmd_info["command"])
    
    # æ ¼å¼åŒ–è¾“å‡º
    return {
        "summary": f"âŒ å½“å‰è¿›åº¦ {progress}, è¿˜éœ€è¡¥é½ {len(missing_fields)} ä¸ªå­—æ®µ{warning}",
        "commands_text": "\n".join(commands[:5]),  # æœ€å¤šæ˜¾ç¤º 5 æ¡
        "progress": progress,
        "critical_text": format_priority_items(priority_groups["critical"]),
        "high_text": format_priority_items(priority_groups["high"]),
        "medium_text": format_priority_items(priority_groups["medium"]),
        "next_action": f"ğŸ“‹ è¯·ç»§ç»­ä¸Šä¼ å›¾è¡¨è¡¥é½å‰©ä½™ {len(missing_fields)} ä¸ªå­—æ®µ(æ”¯æŒå¤šæ¬¡ä¸Šä¼ ç´¯ç§¯)",
        "merge_log": format_merge_history(merge_history)
    }


def suggest_command(field_path: str, symbol: str) -> dict:
    """æ ¹æ®å­—æ®µè·¯å¾„å»ºè®®å‘½ä»¤"""
    # å¤„ç†åµŒå¥—å­—æ®µåï¼ˆå¦‚ "gamma_metrics.vol_trigger"ï¼‰
    field_name = field_path.split('.')[-1] if '.' in field_path else field_path
    
    command_map = {
        "vol_trigger": {
            "command": f"!trigger {symbol} 60",
            "description": "Gamma è§¦å‘çº¿",
            "priority": "critical"
        },
        "net_gex": {
            "command": f"!gexn {symbol} 60 98",
            "description": "å‡€ Gamma æ•å£",
            "priority": "critical"
        },
        "net_gex_sign": {
            "command": f"!gexn {symbol} 60 98",
            "description": "å‡€ Gamma ç¬¦å·",
            "priority": "critical"
        },
        "spot_vs_trigger": {
            "command": f"!trigger {symbol} 60",
            "description": "ç°ä»·ç›¸å¯¹è§¦å‘çº¿",
            "priority": "critical"
        },
        "call_wall": {
            "command": f"!gexr {symbol} 25 7w",
            "description": "Call å¢™ä½",
            "priority": "high"
        },
        "put_wall": {
            "command": f"!gexr {symbol} 25 7w",
            "description": "Put å¢™ä½",
            "priority": "high"
        },
        "major_wall": {
            "command": f"!gexr {symbol} 25 7w",
            "description": "ä¸»å¢™ä½",
            "priority": "high"
        },
        "major_wall_type": {
            "command": f"!gexr {symbol} 25 7w",
            "description": "ä¸»å¢™ç±»å‹",
            "priority": "high"
        },
        "gap_distance_dollar": {
            "command": f"!gexr {symbol} 25 7w",
            "description": "è·³å¢™è·ç¦»ï¼ˆç¾å…ƒï¼‰",
            "priority": "high"
        },
        "gap_distance_em1_multiple": {
            "command": f"!gexr {symbol} 25 7w",
            "description": "è·³å¢™è·ç¦»ï¼ˆEM1å€æ•°ï¼‰",
            "priority": "high"
        },
        "cluster_strength_ratio": {
            "command": f"!gexr {symbol} 25 7w",
            "description": "ç°‡å¼ºåº¦æ¯”",
            "priority": "medium"
        },
        "monthly_cluster_override": {
            "command": f"!gexr {symbol} 25 30m",
            "description": "æœˆåº¦ç°‡å ä¼˜",
            "priority": "medium"
        },
        "iv_7d": {
            "command": f"!skew {symbol} ivmid atm 7",
            "description": "7æ—¥ ATM æ³¢åŠ¨ç‡",
            "priority": "high"
        },
        "iv_14d": {
            "command": f"!skew {symbol} ivmid atm 14",
            "description": "14æ—¥ ATM æ³¢åŠ¨ç‡",
            "priority": "high"
        },
        "iv_source": {
            "command": f"!skew {symbol} ivmid atm 7",
            "description": "IV æ•°æ®æº",
            "priority": "high"
        },
        "dex_same_dir_pct": {
            "command": f"!dexn {symbol} 25 14w",
            "description": "DEX æ–¹å‘ä¸€è‡´æ€§",
            "priority": "medium"
        },
        "vanna_dir": {
            "command": f"!vanna {symbol} ntm 60 m",
            "description": "Vanna æ–¹å‘",
            "priority": "medium"
        },
        "vanna_confidence": {
            "command": f"!vanna {symbol} ntm 60 m",
            "description": "Vanna ç½®ä¿¡åº¦",
            "priority": "medium"
        },
        "iv_path": {
            "command": f"!term {symbol} 60",
            "description": "IV è·¯å¾„è¶‹åŠ¿",
            "priority": "medium"
        },
        "iv_path_confidence": {
            "command": f"!term {symbol} 60",
            "description": "IV è·¯å¾„ç½®ä¿¡åº¦",
            "priority": "medium"
        }
    }
    
    return command_map.get(field_name, {
        "command": f"!gexr {symbol} 25 7w",  # é»˜è®¤å‘½ä»¤
        "description": field_path,
        "priority": "medium"
    })


def format_priority_items(items: list) -> str:
    """æ ¼å¼åŒ–ä¼˜å…ˆçº§åˆ—è¡¨"""
    if not items:
        return "æ— "
    
    result = []
    for i, item in enumerate(items, 1):
        result.append(
            f"{i}. **{item['å­—æ®µ']}**\n"
            f"   - å‘½ä»¤: `{item['å‘½ä»¤']}`\n"
            f"   - è¯´æ˜: {item['è¯´æ˜']}"
        )
    return "\n\n".join(result)


def format_merge_history(history: list) -> str:
    """æ ¼å¼åŒ–åˆå¹¶å†å²"""
    if not history:
        return "æ— å†å²è®°å½•"
    
    lines = []
    for record in history:
        lines.append(
            f"ç¬¬{record['round']}è½® ({record['timestamp']}): "
            f"{record['action']}, "
            f"æ–°å¢ {record.get('fields_added', 0)} ä¸ªå­—æ®µ"
        )
    return "\n".join(lines)


def extract_symbol(data: dict) -> str:
    """æå–è‚¡ç¥¨ä»£ç """
    target = get_target_dict(data)
    return target.get("symbol", data.get("symbol", "UNKNOWN"))


def count_valid_fields(data: dict) -> int:
    """ç»Ÿè®¡æœ‰æ•ˆå­—æ®µæ•°é‡"""
    target = get_target_dict(data)
    return count_valid_fields_in_dict(target)