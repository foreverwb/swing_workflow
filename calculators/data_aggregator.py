"""
æ•°æ®èšåˆèŠ‚ç‚¹ - CODE_AGGREGATOR
æ”¯æŒå¤šæ¬¡ä¸Šä¼ å›¾è¡¨çš„å¢é‡åˆå¹¶å’ŒçŠ¶æ€ç®¡ç†
"""

import json
from typing import Dict, List, Tuple, Any
from datetime import datetime
from utils.logger import setup_logger

logger = setup_logger(__name__)


class DataAggregator:
    """
    æ•°æ®èšåˆå™¨ - æ”¯æŒå¢é‡è¡¥é½
    
    æ ¸å¿ƒåŠŸèƒ½:
    1. æ™ºèƒ½å¢é‡åˆå¹¶: å¤šæ¬¡ä¸Šä¼ è‡ªåŠ¨ç´¯ç§¯æ•°æ®
    2. å­—æ®µçº§è¿½è¸ª: è®°å½•æ¯ä¸ªå­—æ®µçš„æ¥æºå’Œè´¨é‡
    3. é˜²æ­¢è¦†ç›–: æœ‰æ•ˆæ•°æ®ä¸ä¼šè¢«æ— æ•ˆæ•°æ®è¦†ç›–
    4. è‡ªåŠ¨å®Œæˆ: è¾¾åˆ° 22/22 è‡ªåŠ¨è¿›å…¥åˆ†ææµç¨‹
    """
    
    def __init__(self):
        # ä¼šè¯çŠ¶æ€ç¼“å­˜ (æ¨¡æ‹Ÿ Dify ä¼šè¯å˜é‡)
        self.session_state = {
            "first_parse_data": "",      # é¦–æ¬¡è§£ææ•°æ®
            "current_symbol": "",         # å½“å‰è‚¡ç¥¨ä»£ç 
            "data_status": "initial",     # æ•°æ®çŠ¶æ€
            "missing_count": 0            # ç¼ºå¤±å­—æ®µæ•°
        }
    
    def aggregate(self, 
                  current_data: Dict,
                  cached_first_data: str = "",
                  cached_symbol: str = "",
                  cached_status: str = "initial") -> Tuple[Dict, Dict]:
        """
        ä¸»èšåˆå‡½æ•°
        
        Args:
            current_data: Agent 3 å½“å‰è§£æç»“æœ
            cached_first_data: ç¼“å­˜çš„é¦–æ¬¡æ•°æ® (JSON å­—ç¬¦ä¸²)
            cached_symbol: ç¼“å­˜çš„è‚¡ç¥¨ä»£ç 
            cached_status: ç¼“å­˜çš„æ•°æ®çŠ¶æ€
        
        Returns:
            (merged_data, session_updates)
            - merged_data: èšåˆåçš„å®Œæ•´æ•°æ®
            - session_updates: éœ€è¦æ›´æ–°çš„ä¼šè¯å˜é‡
        """
        logger.info("å¼€å§‹æ•°æ®èšåˆ")
        
        # æå–å½“å‰æ•°æ®
        symbol = self._extract_symbol(current_data)
        current_status = current_data.get("status", "missing_data")
        
        # === æ ¸å¿ƒé€»è¾‘ 1: åˆ¤æ–­æ˜¯å¦ç´¯ç§¯æ¨¡å¼ ===
        is_accumulation, judgment = self._judge_accumulation_mode(
            current_data=current_data,
            cached_first_data=cached_first_data,
            cached_symbol=cached_symbol,
            cached_status=cached_status
        )
        
        logger.info(f"ç´¯ç§¯æ¨¡å¼: {is_accumulation}, åŸå› : {judgment}")
        
        # === æ ¸å¿ƒé€»è¾‘ 2: å¢é‡åˆå¹¶ ===
        if is_accumulation:
            if not cached_first_data:
                # ç¬¬ä¸€æ¬¡ä¸Šä¼ 
                merged_data = current_data
                merge_history = [{
                    "round": 1,
                    "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "fields_added": self._count_valid_fields(current_data),
                    "action": "é¦–æ¬¡è§£æ"
                }]
                last_merge_failed = False
            else:
                # ç¬¬ N æ¬¡ä¸Šä¼  - æ‰§è¡Œå¢é‡åˆå¹¶
                first_data = json.loads(cached_first_data)
                merged_data, merge_info = self._smart_merge(first_data, current_data)
                
                # æ›´æ–°åˆå¹¶å†å²
                history = first_data.get("_merge_history", [])
                history.append({
                    "round": len(history) + 1,
                    "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "fields_added": merge_info["new_fields_count"],
                    "fields_updated": merge_info.get("updated_fields_count", 0),
                    "action": "å¢é‡è¡¥é½" if not merge_info.get("merge_failed") else "åˆå¹¶å¤±è´¥",
                    "failure_reason": merge_info.get("failure_reason", "")
                })
                merged_data["_merge_history"] = history
                merge_history = history
                last_merge_failed = merge_info.get("merge_failed", False)
        else:
            # æ–°ä»»åŠ¡ (Symbol å˜åŒ–)
            merged_data = current_data
            merge_history = [{
                "round": 1,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "fields_added": self._count_valid_fields(current_data),
                "action": "æ–°ä»»åŠ¡å¼€å§‹"
            }]
            last_merge_failed = False
        
        # === æ ¸å¿ƒé€»è¾‘ 3: ä¸‰çº§éªŒè¯ (åŸºäºåˆå¹¶åçš„æ•°æ®) ===
        validation_result = self._enhanced_validation(merged_data)
        
        # æ›´æ–°çŠ¶æ€
        if validation_result["is_complete"]:
            final_status = "ready"
            merged_data["status"] = "data_ready"
        else:
            final_status = "awaiting_data"
            merged_data["status"] = "missing_data"
        
        # ç”Ÿæˆè¡¥é½æŒ‡å¼•
        missing_fields = validation_result["missing_fields"]
        guide = self._generate_guide(
            missing_fields=missing_fields,
            merge_history=merge_history,
            total_fields=22,
            last_merge_failed=last_merge_failed
        )
        
        # === æ ¸å¿ƒé€»è¾‘ 4: è¿”å›ç»“æ„åŒ–çš„è¾“å‡º ===
        result_data = {
            **merged_data,
            "validation_summary": validation_result["summary"],
            "åˆ¤æ–­ä¾æ®": judgment,
            "_merge_history": merge_history
        }
        
        # ä¼šè¯å˜é‡æ›´æ–°
        session_updates = {
            "first_parse_data": json.dumps(merged_data) if final_status == "awaiting_data" else "",
            "current_symbol": symbol,
            "data_status": final_status,
            "missing_count": len(missing_fields),
            "user_guide": guide
        }
        
        logger.info(f"èšåˆå®Œæˆ,çŠ¶æ€: {final_status}, ç¼ºå¤±: {len(missing_fields)}")
        
        return result_data, session_updates
    
    def _judge_accumulation_mode(self, 
                                  current_data: Dict,
                                  cached_first_data: str,
                                  cached_symbol: str,
                                  cached_status: str) -> Tuple[bool, str]:
        """
        åˆ¤æ–­æ˜¯å¦è¿›å…¥ç´¯ç§¯æ¨¡å¼ (å¢é‡è¡¥é½)
        
        Returns:
            (is_accumulation, reason)
        """
        # æƒ…å†µ 1: æ— ç¼“å­˜ â†’ é¦–æ¬¡è§£æ
        if not cached_first_data or cached_status == "initial":
            return True, "é¦–æ¬¡ä¸Šä¼ ,å¼€å§‹è§£æ"
        
        # æƒ…å†µ 2: Symbol å˜åŒ– â†’ æ–°ä»»åŠ¡
        current_symbol = self._extract_symbol(current_data)
        if current_symbol != cached_symbol:
            return False, f"Symbol å˜åŒ–({cached_symbol}â†’{current_symbol}),å¼€å§‹æ–°ä»»åŠ¡"
        
        # æƒ…å†µ 3: ç¼“å­˜çŠ¶æ€ä¸º ready â†’ å·²å®Œæˆ,ä¸å†ç´¯ç§¯
        if cached_status == "ready":
            return False, "æ•°æ®å·²å®Œæ•´,å¼€å§‹æ–°ä»»åŠ¡"
        
        # æƒ…å†µ 4: ç¼“å­˜ç­‰å¾…è¡¥é½ â†’ ç´¯ç§¯æ¨¡å¼
        if cached_status == "awaiting_data":
            return True, "æ£€æµ‹åˆ°å†å²ç¼“å­˜,è¿›å…¥å¢é‡è¡¥é½æ¨¡å¼"
        
        # é»˜è®¤: ç´¯ç§¯æ¨¡å¼
        return True, "è¿›å…¥ç´¯ç§¯æ¨¡å¼"
    
    def _smart_merge(self, first_data: Dict, new_data: Dict) -> Tuple[Dict, Dict]:
        """
        æ™ºèƒ½å¢é‡åˆå¹¶
        
        æ ¸å¿ƒç‰¹æ€§:
        1. æ£€æµ‹æ–°æ•°æ®æ˜¯å¦ä¸ºç©º
        2. å¦‚æœæ–°æ•°æ®ä¸ºç©º,ç›´æ¥è¿”å›æ—§æ•°æ® (ä¸åˆå¹¶)
        3. è®°å½•åˆå¹¶å¤±è´¥çš„åŸå› 
        """
        merged = first_data.copy()
        
        # æå– targets
        first_targets = self._get_target_dict(first_data)
        new_targets = self._get_target_dict(new_data)
        
        # æ ¸å¿ƒä¿®å¤: æ£€æµ‹æ–°æ•°æ®æ˜¯å¦ä¸ºç©º
        new_valid_count = self._count_valid_fields_in_dict(new_targets)
        
        if new_valid_count == 0:
            # æ–°æ•°æ®ä¸ºç©º,ä¸æ‰§è¡Œåˆå¹¶
            logger.warning("æ–°æ•°æ®æ— æœ‰æ•ˆå­—æ®µ,è·³è¿‡åˆå¹¶")
            merge_info = {
                "new_fields_count": 0,
                "updated_fields_count": 0,
                "merge_failed": True,
                "failure_reason": "æ–°æ•°æ®æ— æœ‰æ•ˆå­—æ®µ(å¯èƒ½è§£æå¤±è´¥)"
            }
            return merged, merge_info
        
        # ç»Ÿè®¡ä¿¡æ¯
        new_fields_count = 0
        updated_fields_count = 0
        
        # åˆå¹¶å„ä¸ª section
        for section in ["gamma_metrics", "directional_metrics", "atm_iv", "walls"]:
            if section not in first_targets:
                first_targets[section] = {}
            
            if section in new_targets:
                for key, new_value in new_targets[section].items():
                    old_value = first_targets[section].get(key)
                    
                    if self._is_valid_value(new_value):
                        if not self._is_valid_value(old_value):
                            first_targets[section][key] = new_value
                            new_fields_count += 1
                        elif old_value != new_value:
                            first_targets[section][key] = new_value
                            updated_fields_count += 1
        
        # åˆå¹¶é¡¶å±‚å­—æ®µ
        for key in ["spot_price", "em1_dollar", "symbol"]:
            old_value = first_targets.get(key)
            new_value = new_targets.get(key)
            
            if self._is_valid_value(new_value):
                if not self._is_valid_value(old_value):
                    first_targets[key] = new_value
                    new_fields_count += 1
                elif old_value != new_value:
                    first_targets[key] = new_value
                    updated_fields_count += 1
        
        # ä¿®å¤: å¦‚æœæ²¡æœ‰ä»»ä½•æ–°å¢æˆ–æ›´æ–°,æ ‡è®°ä¸ºå¤±è´¥
        if new_fields_count == 0 and updated_fields_count == 0:
            logger.warning("åˆå¹¶æœªäº§ç”Ÿä»»ä½•å˜åŒ–,å¯èƒ½æ•°æ®é‡å¤æˆ–è§£æå¤±è´¥")
            merge_info = {
                "new_fields_count": 0,
                "updated_fields_count": 0,
                "merge_failed": True,
                "failure_reason": "æ— æ–°å¢æˆ–æ›´æ–°å­—æ®µ"
            }
            return merged, merge_info
        
        # åˆå¹¶ indices
        if "indices" not in merged:
            merged["indices"] = {}
        
        if "indices" in new_data:
            for index_name in ["spx", "qqq"]:
                if index_name in new_data["indices"]:
                    if index_name not in merged["indices"]:
                        merged["indices"][index_name] = {}
                    
                    for key, new_value in new_data["indices"][index_name].items():
                        old_value = merged["indices"][index_name].get(key)
                        if self._is_valid_value(new_value) and not self._is_valid_value(old_value):
                            merged["indices"][index_name][key] = new_value
        
        # åˆå¹¶æŠ€æœ¯é¢æ•°æ®
        if "technical_analysis" in new_data:
            ta = new_data["technical_analysis"]
            if ta and ta.get("ta_score", 0) > 0:
                merged["technical_analysis"] = ta
        
        # æ›´æ–° targets
        merged["targets"] = first_targets
        
        merge_info = {
            "new_fields_count": new_fields_count,
            "updated_fields_count": updated_fields_count,
            "merge_failed": False
        }
        
        return merged, merge_info
    
    def _enhanced_validation(self, data: Dict) -> Dict:
        """
        ä¸‰çº§éªŒè¯å¢å¼ºç‰ˆ (åŸºäºåˆå¹¶åçš„æ•°æ®)
        
        Returns:
            {
                "is_complete": bool,
                "missing_fields": list,
                "summary": dict
            }
        """
        target = self._get_target_dict(data)
        
        # 22 ä¸ªå¿…éœ€å­—æ®µ
        required_fields = {
            # é¡¶å±‚å­—æ®µ
            "spot_price": (target, "spot_price"),
            "em1_dollar": (target, "em1_dollar"),
            
            # walls
            "walls.call_wall": (target.get("walls", {}), "call_wall"),
            "walls.put_wall": (target.get("walls", {}), "put_wall"),
            "walls.major_wall": (target.get("walls", {}), "major_wall"),
            "walls.major_wall_type": (target.get("walls", {}), "major_wall_type"),
            
            # gamma_metrics
            "gamma_metrics.gap_distance_dollar": (target.get("gamma_metrics", {}), "gap_distance_dollar"),
            "gamma_metrics.gap_distance_em1_multiple": (target.get("gamma_metrics", {}), "gap_distance_em1_multiple"),
            "gamma_metrics.cluster_strength_ratio": (target.get("gamma_metrics", {}), "cluster_strength_ratio"),
            "gamma_metrics.net_gex": (target.get("gamma_metrics", {}), "net_gex"),
            "gamma_metrics.net_gex_sign": (target.get("gamma_metrics", {}), "net_gex_sign"),
            "gamma_metrics.vol_trigger": (target.get("gamma_metrics", {}), "vol_trigger"),
            "gamma_metrics.spot_vs_trigger": (target.get("gamma_metrics", {}), "spot_vs_trigger"),
            "gamma_metrics.monthly_cluster_override": (target.get("gamma_metrics", {}), "monthly_cluster_override"),
            
            # directional_metrics
            "directional_metrics.dex_same_dir_pct": (target.get("directional_metrics", {}), "dex_same_dir_pct"),
            "directional_metrics.vanna_dir": (target.get("directional_metrics", {}), "vanna_dir"),
            "directional_metrics.vanna_confidence": (target.get("directional_metrics", {}), "vanna_confidence"),
            "directional_metrics.iv_path": (target.get("directional_metrics", {}), "iv_path"),
            "directional_metrics.iv_path_confidence": (target.get("directional_metrics", {}), "iv_path_confidence"),
            
            # atm_iv
            "atm_iv.iv_7d": (target.get("atm_iv", {}), "iv_7d"),
            "atm_iv.iv_14d": (target.get("atm_iv", {}), "iv_14d"),
            "atm_iv.iv_source": (target.get("atm_iv", {}), "iv_source"),
        }
        
        # æ£€æŸ¥ç¼ºå¤±å­—æ®µ
        missing_fields = []
        for field_path, (parent_dict, key) in required_fields.items():
            value = parent_dict.get(key) if isinstance(parent_dict, dict) else None
            if not self._is_valid_value(value):
                missing_fields.append({
                    "field": field_path,
                    "current_value": value
                })
        
        total_required = len(required_fields)
        provided = total_required - len(missing_fields)
        completion_rate = int((provided / total_required) * 100)
        
        is_complete = len(missing_fields) == 0
        
        return {
            "is_complete": is_complete,
            "missing_fields": missing_fields,
            "summary": {
                "total_required": total_required,
                "provided": provided,
                "missing_count": len(missing_fields),
                "completion_rate": completion_rate
            }
        }
    
    def _generate_guide(self,
                        missing_fields: list,
                        merge_history: list,
                        total_fields: int,
                        last_merge_failed: bool = False) -> Dict:
        """ç”Ÿæˆæ™ºèƒ½è¡¥é½æŒ‡å¼•"""
        if not missing_fields:
            return {
                "summary": "âœ… æ•°æ®å®Œæ•´,æ— éœ€è¡¥é½",
                "commands": [],
                "progress": f"100% ({total_fields}/{total_fields})",
                "next_action": "è¿›å…¥åˆ†ææµç¨‹"
            }
        
        provided_count = total_fields - len(missing_fields)
        progress = f"{int((provided_count/total_fields)*100)}% ({provided_count}/{total_fields})"
        
        warning = ""
        if last_merge_failed:
            warning = "\n\nâš ï¸ **è­¦å‘Š**: ä¸Šæ¬¡ä¸Šä¼ çš„æ•°æ®æœªèƒ½æˆåŠŸè¯†åˆ«,è¯·ç¡®ä¿:\n" \
                      "1. å›¾ç‰‡æ¸…æ™°å®Œæ•´\n" \
                      "2. åŒ…å«ç›®æ ‡è‚¡ç¥¨çš„æœŸæƒæ•°æ®\n" \
                      "3. å‘½ä»¤æ‰§è¡Œç»“æœå®Œæ•´æ˜¾ç¤º"
        
        # ç”Ÿæˆå‘½ä»¤å»ºè®®
        commands = []
        for item in missing_fields[:5]:  # æœ€å¤šæ˜¾ç¤º 5 æ¡
            field_path = item["field"]
            cmd_info = self._suggest_command(field_path)
            commands.append(cmd_info["command"])
        
        return {
            "summary": f"âŒ å½“å‰è¿›åº¦ {progress}, è¿˜éœ€è¡¥é½ {len(missing_fields)} ä¸ªå­—æ®µ{warning}",
            "commands": commands,
            "progress": progress,
            "next_action": f"ğŸ“‹ è¯·ç»§ç»­ä¸Šä¼ å›¾è¡¨è¡¥é½å‰©ä½™ {len(missing_fields)} ä¸ªå­—æ®µ(æ”¯æŒå¤šæ¬¡ä¸Šä¼ ç´¯ç§¯)"
        }
    
    def _suggest_command(self, field_path: str) -> dict:
        """æ ¹æ®å­—æ®µè·¯å¾„å»ºè®®å‘½ä»¤"""
        command_map = {
            "gamma_metrics.vol_trigger": {
                "command": "!trigger SYMBOL 60",
                "description": "Gamma è§¦å‘çº¿"
            },
            "gamma_metrics.net_gex": {
                "command": "!gexn SYMBOL 60 98",
                "description": "å‡€ Gamma æ•å£"
            },
            "walls.call_wall": {
                "command": "!gexr SYMBOL 25 7w",
                "description": "Call å¢™ä½"
            },
            "atm_iv.iv_7d": {
                "command": "!skew SYMBOL ivmid atm 7",
                "description": "7æ—¥ ATM æ³¢åŠ¨ç‡"
            },
            "directional_metrics.dex_same_dir_pct": {
                "command": "!dexn SYMBOL 25 14w",
                "description": "DEX æ–¹å‘ä¸€è‡´æ€§"
            },
        }
        
        return command_map.get(field_path, {
            "command": "!gexr SYMBOL 25 7w",
            "description": field_path
        })
    
    def _extract_symbol(self, data: Dict) -> str:
        """æå–è‚¡ç¥¨ä»£ç """
        target = self._get_target_dict(data)
        return target.get("symbol", data.get("symbol", "UNKNOWN"))
    
    def _count_valid_fields(self, data: Dict) -> int:
        """ç»Ÿè®¡æœ‰æ•ˆå­—æ®µæ•°é‡"""
        target = self._get_target_dict(data)
        count = 0
        
        for section in ["gamma_metrics", "directional_metrics", "atm_iv", "walls"]:
            if section in target and isinstance(target[section], dict):
                for value in target[section].values():
                    if self._is_valid_value(value):
                        count += 1
        
        for key in ["spot_price", "em1_dollar"]:
            if self._is_valid_value(target.get(key)):
                count += 1
        
        return count
    
    def _count_valid_fields_in_dict(self, target_dict: dict) -> int:
        """ç»Ÿè®¡å­—å…¸ä¸­çš„æœ‰æ•ˆå­—æ®µæ•°é‡"""
        count = 0
        
        for section in ["gamma_metrics", "directional_metrics", "atm_iv", "walls"]:
            if section in target_dict and isinstance(target_dict[section], dict):
                for value in target_dict[section].values():
                    if self._is_valid_value(value):
                        count += 1
        
        for key in ["spot_price", "em1_dollar"]:
            if self._is_valid_value(target_dict.get(key)):
                count += 1
        
        return count
    
    def _is_valid_value(self, value: Any) -> bool:
        """åˆ¤æ–­å€¼æ˜¯å¦æœ‰æ•ˆ(éç¼ºå¤±)"""
        if value is None:
            return False
        if value == -999:
            return False
        if value in ["N/A", "æ•°æ®ä¸è¶³", "", "unknown"]:
            return False
        return True
    
    def _get_target_dict(self, data: dict) -> dict:
        """
        æå– targets å­—å…¸ (å¢å¼ºé˜²å¾¡æ€§)
        
        è¿”å›ä¼˜å…ˆçº§:
        1. å¦‚æœ targets æ˜¯éç©ºå­—å…¸ â†’ ç›´æ¥è¿”å›
        2. å¦‚æœ targets æ˜¯éç©ºåˆ—è¡¨ â†’ è¿”å›ç¬¬ä¸€ä¸ªå…ƒç´ 
        3. å¦‚æœ targets ä¸ºç©ºæˆ–ç¼ºå¤± â†’ è¿”å›ç©ºå­—å…¸(ä½†ä¼šåœ¨æ—¥å¿—ä¸­è­¦å‘Š)
        """
        targets = data.get("targets")
        
        # æƒ…å†µ 1: None æˆ–ç¼ºå¤±
        if targets is None:
            logger.warning("targets å­—æ®µç¼ºå¤±")
            return {}
        
        # æƒ…å†µ 2: ç©ºåˆ—è¡¨
        if isinstance(targets, list):
            if not targets:
                logger.warning("targets æ˜¯ç©ºåˆ—è¡¨")
                return {}
            return targets[0]
        
        # æƒ…å†µ 3: å­—å…¸
        if isinstance(targets, dict):
            if not targets:
                logger.warning("targets æ˜¯ç©ºå­—å…¸")
            return targets
        
        # æƒ…å†µ 4: å…¶ä»–ç±»å‹(å¼‚å¸¸)
        logger.error(f"targets ç±»å‹å¼‚å¸¸ - {type(targets)}")
        return {}